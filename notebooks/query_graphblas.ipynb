{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import graphblas as gb\n",
    "import numpy as np\n",
    "import pronto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Step 1. Load an ontology using Pronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_ontology = pronto.Ontology(\"../data/out/go.obo\")\n",
    "#go_ontology = pronto.Ontology(\"../tests/resources/dummy_ontology.obo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Step 8. Create classes to organize the code\n",
    "\n",
    "Objects\n",
    "\n",
    "- LUTs\n",
    "- properties of graph\n",
    "  - number of nodes\n",
    "  - number of edges\n",
    "  - relation types\n",
    "- Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "###  Lookup Tables Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookUpTables:\n",
    "    def __init__(self, ontology):\n",
    "        self.__lut_term_to_index = self.__create_lut_term_index(ontology=ontology)\n",
    "        self.__lut_index_to_term = self.__create_lut_nodes(lookup_table=self.__lut_term_to_index)\n",
    "        self.__lut_term_to_description = self.__create_lut_term_description(ontology=ontology)\n",
    "        self.__lut_description_to_term = self.__create_lut_description_term(ontology=ontology)\n",
    "\n",
    "    # Private methods\n",
    "    def __create_lut_term_index(self, ontology)-> dict[str, int]:\n",
    "        terms = [term for term in ontology.terms() if not term.obsolete]\n",
    "        terms.sort(key=lambda term: term.id)\n",
    "        return {term.id: idx for idx, term in enumerate(terms)}\n",
    "    \n",
    "    def __create_lut_nodes(self, lookup_table):\n",
    "        return list(lookup_table.keys())\n",
    "    \n",
    "    def __create_lut_term_description(self, ontology):\n",
    "        return {term.id: term.name for term in ontology.terms() if not term.obsolete}\n",
    "    \n",
    "    def __create_lut_description_term(self, ontology):\n",
    "        return {term.name: term.id for term in ontology.terms() if not term.obsolete}\n",
    "    \n",
    "\n",
    "    # Public methods\n",
    "    def get_lut_term_to_index(self):\n",
    "        return self.__lut_term_to_index\n",
    "    \n",
    "    def get_lut_index_to_term(self):\n",
    "        return self.__lut_index_to_term\n",
    "    \n",
    "    def get_lut_term_to_description(self):\n",
    "        return self.__lut_term_to_description\n",
    "    \n",
    "    def get_lut_description_to_term(self):\n",
    "        return self.__lut_description_to_term\n",
    "\n",
    "\n",
    "    def term_to_index(self, terms: str | list):\n",
    "        if isinstance(terms, str):\n",
    "            # Single term ID\n",
    "            return self.__lut_term_to_index[terms]\n",
    "        elif isinstance(terms, list):\n",
    "            # List of term IDs\n",
    "            return [self.__lut_term_to_index[term] for term in terms]\n",
    "\n",
    "    def index_to_term(self, indexes: int | list):\n",
    "        if isinstance(indexes, int):\n",
    "            # Single index\n",
    "            return self.__lut_index_to_term[indexes]\n",
    "        elif isinstance(indexes, list):\n",
    "            # List of indexes\n",
    "            return [self.__lut_index_to_term[idx] for idx in indexes]\n",
    "        elif isinstance(indexes, np.ndarray):\n",
    "            # NumPy array of indices (vectorized lookup)\n",
    "            return [self.__lut_index_to_term[idx] for idx in indexes.tolist()]\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Expected int, list[int], or np.ndarray, got {type(indexes).__name__}.\"\n",
    "            )\n",
    "\n",
    "    def term_to_description(self, terms: str | list):\n",
    "        if isinstance(terms, str):\n",
    "            # Single term ID\n",
    "            return self.__lut_term_to_description[terms]\n",
    "        elif isinstance(terms, list):\n",
    "            # List of term IDs\n",
    "            return [self.__lut_term_to_description[term] for term in terms]\n",
    "\n",
    "    def description_to_term(self, descriptions: str | list):\n",
    "        # TODO: improve with Levenstein or Regex expresions\n",
    "        if isinstance(descriptions, str):\n",
    "            # Single description\n",
    "            return self.__lut_description_to_term[descriptions]\n",
    "        elif isinstance(descriptions, list):\n",
    "            # List of descriptions\n",
    "            return [self.__lut_description_to_term[term] for term in descriptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LookUpTables given an ontology\n",
    "L = LookUpTables(ontology=go_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x] verified agains Protege\n",
    "\n",
    "list_terms = ['GO:0008150', 'GO:0000017', 'GO:0000001']\n",
    "list_indices = [5099, 10, 0]\n",
    "list_descriptions = ['biological_process', 'alpha-glucoside transport', 'mitochondrion inheritance']\n",
    "\n",
    "\n",
    "print(f\"Indices:\\n\\t{L.term_to_index(list_terms)}\")\n",
    "print(f\"Terms:\\n\\t{L.index_to_term(list_indices)}\")\n",
    "print(f\"Descriptions:\\n\\t{L.term_to_description(list_terms)}\")\n",
    "print(f\"Terms from descriptions:\\n\\t{L.description_to_term(list_descriptions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "from collections import defaultdict\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, ontology, lookup_tables):\n",
    "        # Core elements\n",
    "        self.lookup_tables = lookup_tables\n",
    "        self.nodes_container = self.lookup_tables.get_lut_index_to_term()\n",
    "        self.edges_container = self.populate_index_containers(ontology=ontology,\n",
    "                                                              lut_term_index=self.lookup_tables.get_lut_term_to_index()\n",
    "                                                              )\n",
    "        self.matrices_container = self.create_multiple_matrices(edge_container=self.edges_container,\n",
    "                                                             nrows=len(self.nodes_container),\n",
    "                                                             ncols=len(self.nodes_container))\n",
    " \n",
    "        # Metadata\n",
    "        self.relation_types =  self.get_ontology_relationships(ontology=ontology)\n",
    "        self.number_nodes = self.number_nodes_ontology(ontology=ontology)\n",
    "        self.number_edges = self.edges_container['is_a']['rows'].shape[0]\n",
    "    ## Private methods\n",
    "    def create_edges_index_containers(self, ontology):\n",
    "\n",
    "        \"\"\"    \n",
    "            rows represents sources\n",
    "            cols represents targets\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract explicit relationships \n",
    "        relationships = self.get_ontology_relationships(ontology)\n",
    "\n",
    "        # Append implicit relationship 'is_a'\n",
    "        relationships.append('is_a')\n",
    "\n",
    "        # create edge container\n",
    "        edge_container = {rel: {'rows': [], 'cols':[] } for rel in relationships}\n",
    "\n",
    "        return edge_container\n",
    "    \n",
    "\n",
    "    def populate_index_containers(self, ontology, lut_term_index):\n",
    "        # create an empty edges container\n",
    "        edge_container = self.create_edges_index_containers(ontology)\n",
    "\n",
    "        # main loop to extract terms and relationships\n",
    "        for idx, term in enumerate(ontology.terms()):\n",
    "            # ignore obsolete terms\n",
    "            if term.obsolete:\n",
    "                continue\n",
    "            \n",
    "            # extract super classes for each term ('is_a' relationship)\n",
    "            for subclass in term.subclasses(with_self=False, distance=1):\n",
    "                if subclass.obsolete:\n",
    "                    continue\n",
    "                edge_container['is_a']['rows'].append(\n",
    "                    self.lookup_tables.term_to_index(subclass.id)\n",
    "                )\n",
    "                edge_container['is_a']['cols'].append(\n",
    "                    self.lookup_tables.term_to_index(term.id)\n",
    "                )\n",
    "                \n",
    "            # extract explict relationships (i.e., 'part_of')\n",
    "            for rel, targets in term.relationships.items():\n",
    "                for target in targets:\n",
    "                    if target.obsolete:\n",
    "                        continue\n",
    "                    edge_container[rel.id]['rows'].append(\n",
    "                        self.lookup_tables.term_to_index(term.id)\n",
    "                    )\n",
    "                    edge_container[rel.id]['cols'].append(\n",
    "                        self.lookup_tables.term_to_index(target.id)\n",
    "                    )\n",
    "\n",
    "        # convert lists to numpy arrays\n",
    "        for rel, data in edge_container.items():\n",
    "            data['rows'] = np.array(data['rows'])\n",
    "            data['cols'] = np.array(data['cols'])\n",
    "        \n",
    "        return edge_container\n",
    "\n",
    "    def create_graphblas_matrix(self, rows_indexes, cols_indexes, nrows, ncols, name):\n",
    "        M = gb.Matrix.from_coo(rows=rows_indexes, columns=cols_indexes, values=1.0, nrows=nrows, ncols=ncols, dtype=bool, name=name)\n",
    "        return M\n",
    "\n",
    "    def create_multiple_matrices(self, edge_container, nrows, ncols):\n",
    "        matrices = {}\n",
    "        for relation, indexes in edge_container.items():\n",
    "            M = self.create_graphblas_matrix(rows_indexes=indexes['rows'],\n",
    "                                        cols_indexes=indexes['cols'],\n",
    "                                        nrows=nrows,\n",
    "                                        ncols=ncols,\n",
    "                                        name=relation)\n",
    "\n",
    "            matrices[relation] = M\n",
    "        return matrices\n",
    "    \n",
    "    # Function to get all name of relationships\n",
    "    def get_ontology_relationships(self, ontology):\n",
    "        set_relations = set()\n",
    "        for term in ontology.terms():\n",
    "            for rel in term.relationships:\n",
    "                set_relations.add(rel.id)\n",
    "\n",
    "        return sorted(set_relations)\n",
    "\n",
    "\n",
    "    # Calculate the number of nodes for the current ontology\n",
    "    def number_nodes_ontology(self, ontology):\n",
    "        return len([term for term in ontology.terms() if not term.obsolete])\n",
    "        \n",
    "    ## Public methods\n",
    "\n",
    "    # Generates a one-hot encoded vector for a given index\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def one_hot_vector(self, index: int) -> gb.Vector:\n",
    "        return gb.Vector.from_coo([index], [1], size=self.number_nodes, dtype=int)\n",
    "\n",
    "    # -- get_children(term_id, include_self=False)\n",
    "    def get_children(self, term_id, include_self=False):\n",
    "        # validate and resolve the index\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "\n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "\n",
    "        # Initialize a one-hot vector for the term node\n",
    "        vector_node = self.one_hot_vector(index=index)\n",
    "\n",
    "        # Propagate to children using matrix-vector multiplication\n",
    "        children_vec = (self.matrices_container['is_a'] @ vector_node).new()\n",
    "\n",
    "        # Optionally include the node itself\n",
    "        if include_self:\n",
    "            children_vec[index] = True\n",
    "\n",
    "        # translate indexes to terms\n",
    "        terms = [term for term in children_vec]\n",
    "        \n",
    "        return self.lookup_tables.index_to_term(terms)\n",
    "    \n",
    "    # -- get_parents(term_id, include_self=False)\n",
    "    def get_parents(self, term_id, include_self=False):\n",
    "        # validate and resolve the index\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "\n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "\n",
    "        # Initialize a one-hot vector for the term node\n",
    "        vector_node = self.one_hot_vector(index=index)\n",
    "\n",
    "        # Propagate to children using matrix-vector multiplication\n",
    "        parent_vec = (self.matrices_container['is_a'].T @ vector_node).new()\n",
    "\n",
    "        # Optionally include the node itself\n",
    "        if include_self:\n",
    "            parent_vec[index] = True\n",
    "\n",
    "        # translate indexes to terms\n",
    "        terms = [term for term in parent_vec]\n",
    "        \n",
    "        return self.lookup_tables.index_to_term(terms)\n",
    "\n",
    "    # -- get_root()\n",
    "    def get_root(self):\n",
    "    \n",
    "        matrix = self.matrices_container['is_a'].T\n",
    "\n",
    "        # 1. Compute the number of incoming edges per node (column-wise sum)\n",
    "        col_sums_expr = matrix.reduce_columnwise(gb.binary.plus)\n",
    "\n",
    "        # 2. Materialize the VectorExpression\n",
    "        col_sums_vec = col_sums_expr.new()\n",
    "\n",
    "        # 3. Extract non-zero indices and their counts\n",
    "        indices, values = col_sums_vec.to_coo()\n",
    "\n",
    "        # 4. Create dense array of incoming edge counts\n",
    "        col_sums_np = np.zeros(matrix.ncols, dtype=np.int64)\n",
    "        col_sums_np[indices] = values\n",
    "\n",
    "        # 5. Roots = nodes with zero incoming edges\n",
    "        roots = np.where(col_sums_np == 0)[0]\n",
    "\n",
    "        return self.lookup_tables.index_to_term(roots)\n",
    "    \n",
    "    def _traverse_graph(self, term_id, adjacency_matrix, distance=None, include_self=False):\n",
    "        \"\"\"\n",
    "        Generalized function to traverse a graph in either direction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The starting term ID.\n",
    "        adjacency_matrix : gb.Matrix\n",
    "            Adjacency matrix to traverse (forward for descendants, transposed for ancestors).\n",
    "        distance : int or None\n",
    "            Maximum distance to traverse. None means unlimited.\n",
    "        include_self : bool\n",
    "            Whether to include the starting node in the result.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs reached.\n",
    "        \"\"\"\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "        \n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "        current_vector = self.one_hot_vector(index=index)\n",
    "        visited = set()\n",
    "        \n",
    "        if include_self:\n",
    "            visited.add(index)\n",
    "        \n",
    "        while current_vector.nvals != 0 and distance != 0:\n",
    "            next_vector = gb.Vector(dtype=int, size=adjacency_matrix.nrows)\n",
    "            next_vector << gb.semiring.plus_times(adjacency_matrix @ current_vector)  # forward or transposed depends on matrix\n",
    "            \n",
    "            next_indices = set(next_vector.to_coo()[0])\n",
    "            next_indices.difference_update(visited)\n",
    "            \n",
    "            if not next_indices:\n",
    "                break\n",
    "            \n",
    "            visited.update(next_indices)\n",
    "            current_vector = gb.Vector.from_coo(list(next_indices), [1]*len(next_indices), size=adjacency_matrix.nrows)\n",
    "\n",
    "            if distance is not None:\n",
    "                distance -= 1\n",
    "\n",
    "        return self.lookup_tables.index_to_term(list(visited))\n",
    "\n",
    "\n",
    "    # Public API functions\n",
    "    def get_ancestors(self, term_id, distance=None, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container['is_a'].T  # transpose for ancestors\n",
    "        return self._traverse_graph(term_id, adjacency_matrix, distance, include_self)\n",
    "\n",
    "    def get_descendants(self, term_id, distance=None, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container['is_a']  # normal direction for descendants\n",
    "        return self._traverse_graph(term_id, adjacency_matrix, distance, include_self)\n",
    "\n",
    "    def _traverse_graph_with_distance(self, term_id, adjacency_matrix, include_self=False):\n",
    "        \"\"\"\n",
    "        Generalized function to traverse a graph and return nodes with distance from start.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The starting term ID.\n",
    "        adjacency_matrix : gb.Matrix\n",
    "            Adjacency matrix to traverse (forward for descendants, transposed for ancestors).\n",
    "        include_self : bool\n",
    "            Whether to include the starting node with distance 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[int, int]]\n",
    "            List of tuples (node_index, distance_from_start)\n",
    "        \"\"\"\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "        \n",
    "        start_index = self.lookup_tables.term_to_index(term_id)\n",
    "        current_vector = self.one_hot_vector(index=start_index)\n",
    "        \n",
    "        distances = {}  # {node_index: distance}\n",
    "        distance_counter = 0\n",
    "        \n",
    "        if include_self:\n",
    "            distances[start_index] = 0\n",
    "\n",
    "        while current_vector.nvals != 0:\n",
    "            next_vector = gb.Vector(dtype=int, size=adjacency_matrix.nrows)\n",
    "            next_vector << gb.semiring.plus_times(adjacency_matrix @ current_vector)\n",
    "            \n",
    "            next_indices = set(next_vector.to_coo()[0])\n",
    "            # remove already visited nodes\n",
    "            next_indices.difference_update(distances.keys())\n",
    "            \n",
    "            if not next_indices:\n",
    "                break\n",
    "            \n",
    "            distance_counter += 1\n",
    "            for idx in next_indices:\n",
    "                distances[idx] = distance_counter\n",
    "            \n",
    "            current_vector = gb.Vector.from_coo(list(next_indices), [1]*len(next_indices), size=adjacency_matrix.nrows)\n",
    "\n",
    "        # return as list of tuples\n",
    "        return [(self.lookup_tables.index_to_term(int(index)), distance) for index, distance in distances.items()]\n",
    "\n",
    "\n",
    "    # Public API functions\n",
    "    def get_ancestors_with_distance(self, term_id, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container['is_a'].T  # transpose for ancestors\n",
    "        return self._traverse_graph_with_distance(term_id, adjacency_matrix, include_self)\n",
    "\n",
    "    def get_descendants_with_distance(self, term_id, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container['is_a']  # normal direction for descendants\n",
    "        return self._traverse_graph_with_distance(term_id, adjacency_matrix, include_self)\n",
    "    \n",
    "    def get_common_ancestors(self, node_ids):\n",
    "        \"\"\"\n",
    "        Return the common ancestors of a list of terms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : List[str]\n",
    "            List of starting term IDs.\n",
    "        include_self : bool\n",
    "            Whether to include the starting nodes themselves in the ancestor sets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs that are common ancestors to all input terms.\n",
    "        \"\"\"\n",
    "        if not node_ids:\n",
    "            return []\n",
    "\n",
    "        # get ancestors for the first node\n",
    "        common_ancestors = set(self.get_ancestors(node_ids[0], include_self=False))\n",
    "\n",
    "        # intersect with ancestors of the rest\n",
    "        for term_id in node_ids[1:]:\n",
    "            ancestors = set(self.get_ancestors(term_id, include_self=False))\n",
    "            common_ancestors.intersection_update(ancestors)\n",
    "\n",
    "            # early exit if no common ancestor remains\n",
    "            if not common_ancestors:\n",
    "                return []\n",
    "\n",
    "        return set(common_ancestors)\n",
    "    \n",
    "    def get_lowest_common_ancestors(self, node_ids):\n",
    "        \"\"\"\n",
    "        Return the lowest common ancestor(s) of a list of terms. \n",
    "        Lowest = closest to the given terms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : List[str]\n",
    "            List of starting term IDs.\n",
    "        include_self : bool\n",
    "            Whether to include the starting nodes in ancestor sets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs that are the lowest common ancestors.\n",
    "        \"\"\"\n",
    "        if not node_ids:\n",
    "            return []\n",
    "\n",
    "        # Compute ancestors with distances for the first node\n",
    "        first_ancestors = dict(self.get_ancestors_with_distance(node_ids[0], include_self=False))\n",
    "        common_ancestors = set(first_ancestors.keys())\n",
    "\n",
    "        # Initialize distances dict for LCA calculation\n",
    "        # key: ancestor index, value: max distance from any node\n",
    "        lca_distances = {idx: dist for idx, dist in first_ancestors.items()}\n",
    "\n",
    "        # Process remaining nodes\n",
    "        for term_id in node_ids[1:]:\n",
    "            ancestors_with_distance = dict(self.get_ancestors_with_distance(term_id, include_self=False))\n",
    "            ancestors_set = set(ancestors_with_distance.keys())\n",
    "            common_ancestors.intersection_update(ancestors_set)\n",
    "\n",
    "            # Update max distance for each common ancestor\n",
    "            lca_distances = {idx: max(lca_distances[idx], ancestors_with_distance[idx])\n",
    "                            for idx in common_ancestors}\n",
    "\n",
    "            # Early exit if no common ancestor remains\n",
    "            if not common_ancestors:\n",
    "                return []\n",
    "\n",
    "        if not lca_distances:\n",
    "            return []\n",
    "\n",
    "        # Find the minimum of the maximum distances\n",
    "        min_distance = min(lca_distances.values())\n",
    "\n",
    "        # Return ancestor IDs that have this minimum distance\n",
    "        lowest_common_indices = [idx for idx, dist in lca_distances.items() if dist == min_distance]\n",
    "        return lowest_common_indices\n",
    "        \n",
    "    def get_distance_from_root(self, term_id):\n",
    "        \"\"\"\n",
    "        Calculate the distance from the given term to the root node(s) of the ontology.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The term ID for which to compute the distance from root.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Distance from the term to the root (number of edges).\n",
    "            Returns 0 if the term is a root itself.\n",
    "        \"\"\"\n",
    "        # Validate term\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "\n",
    "        # Get all ancestors with distance\n",
    "        ancestors_with_distance = self.get_ancestors_with_distance(term_id, include_self=True)\n",
    "\n",
    "        if not ancestors_with_distance:\n",
    "            # No ancestors, this term is a root\n",
    "            return 0\n",
    "\n",
    "        # Distance from root = maximum distance in the ancestors path\n",
    "        max_distance = max(distance for _, distance in ancestors_with_distance)\n",
    "\n",
    "        return max_distance\n",
    "    \n",
    "    def get_path_between(self, node_a, node_b):\n",
    "        \"\"\"\n",
    "        Find the shortest path between two nodes in the ontology.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_a : str\n",
    "            Starting term ID.\n",
    "        node_b : str\n",
    "            Ending term ID.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs representing the path from node_a to node_b (inclusive).\n",
    "            Returns empty list if no path exists.\n",
    "        \"\"\"\n",
    "        if node_a not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {node_a}\")\n",
    "        if node_b not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {node_b}\")\n",
    "\n",
    "        # Check if a path exists\n",
    "        if not (self.is_ancestor(node_a, node_b) or self.is_descendant(node_a, node_b)):\n",
    "            return []\n",
    "\n",
    "        # Determine direction\n",
    "        if self.is_ancestor(node_a, node_b):\n",
    "            start, end = node_a, node_b\n",
    "            adjacency_matrix = self.matrices_container['is_a']\n",
    "        else:\n",
    "            start, end = node_b, node_a\n",
    "            adjacency_matrix = self.matrices_container['is_a']\n",
    "\n",
    "        start_idx = self.lookup_tables.term_to_index(start)\n",
    "        end_idx = self.lookup_tables.term_to_index(end)\n",
    "\n",
    "        # BFS to find shortest path\n",
    "        from collections import deque\n",
    "        queue = deque([[start_idx]])\n",
    "        visited = set([start_idx])\n",
    "\n",
    "        while queue:\n",
    "            path = queue.popleft()\n",
    "            current = path[-1]\n",
    "\n",
    "            if current == end_idx:\n",
    "                return self.lookup_tables.index_to_term(path)\n",
    "\n",
    "            # Get children (or parents depending on direction)\n",
    "            neighbors_vec = adjacency_matrix @ self.one_hot_vector(current)\n",
    "            neighbors = neighbors_vec.to_coo()[0]\n",
    "\n",
    "            for n in neighbors:\n",
    "                if n not in visited:\n",
    "                    visited.add(n)\n",
    "                    queue.append(path + [n])\n",
    "\n",
    "        return []\n",
    "    \n",
    "    def is_ancestor(self, ancestor_node, descendant_node):\n",
    "        \"\"\"\n",
    "        Check if `ancestor_node` is an ancestor of `descendant_node`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ancestor_node : str\n",
    "            Candidate ancestor term ID.\n",
    "        descendant_node : str\n",
    "            Candidate descendant term ID.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if `ancestor_node` is an ancestor of `descendant_node`, else False.\n",
    "        \"\"\"\n",
    "        if descendant_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {descendant_node}\")\n",
    "        if ancestor_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {ancestor_node}\")\n",
    "        \n",
    "        # Retrieve ancestors of the descendant\n",
    "        ancestors = set(self.get_ancestors(descendant_node, include_self=False))\n",
    "        return ancestor_node in ancestors\n",
    "\n",
    "\n",
    "    def is_descendant(self, descendant_node, ancestor_node):\n",
    "        \"\"\"\n",
    "        Check if `descendant_node` is a descendant of `ancestor_node`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        descendant_node : str\n",
    "            Candidate descendant term ID.\n",
    "        ancestor_node : str\n",
    "            Candidate ancestor term ID.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if `descendant_node` is a descendant of `ancestor_node`, else False.\n",
    "        \"\"\"\n",
    "        if ancestor_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {ancestor_node}\")\n",
    "        if descendant_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {descendant_node}\")\n",
    "        \n",
    "        # Retrieve descendants of the ancestor\n",
    "        descendants = set(self.get_descendants(ancestor_node, include_self=False))\n",
    "        return descendant_node in descendants\n",
    "    \n",
    "    def get_siblings(self, term_id, include_self: bool = False):\n",
    "        \"\"\"\n",
    "        Retrieve all siblings of a given term (i.e., nodes that share at least one parent).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The term ID whose siblings are to be found.\n",
    "        include_self : bool, optional (default=False)\n",
    "            Whether to include the term itself in the returned set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            List of sibling term IDs.\n",
    "        \"\"\"\n",
    "        # Validate term existence\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "\n",
    "        # Step 1: Get parents of the given term\n",
    "        parents = self.get_parents(term_id, include_self=False)\n",
    "        if not parents:\n",
    "            # No parents means this term is a root -> no siblings\n",
    "            return []\n",
    "\n",
    "        # Step 2: For each parent, get its children\n",
    "        siblings_set = set()\n",
    "        for parent_id in parents:\n",
    "            children = self.get_children(parent_id, include_self=False)\n",
    "            siblings_set.update(children)\n",
    "\n",
    "        # Step 3: Optionally remove the term itself\n",
    "        if not include_self and term_id in siblings_set:\n",
    "            siblings_set.remove(term_id)\n",
    "\n",
    "        # Return as sorted list for deterministic output\n",
    "        return sorted(siblings_set)\n",
    "    \n",
    "    def is_sibling(self, node_a: str, node_b: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if two nodes are siblings (i.e., share at least one common parent).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_a : str\n",
    "            First node (term ID).\n",
    "        node_b : str\n",
    "            Second node (term ID).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if both nodes share at least one parent; False otherwise.\n",
    "        \"\"\"\n",
    "        # Validate existence\n",
    "        lut = self.lookup_tables.get_lut_term_to_index()\n",
    "        if node_a not in lut:\n",
    "            raise KeyError(f\"Unknown term ID: {node_a}\")\n",
    "        if node_b not in lut:\n",
    "            raise KeyError(f\"Unknown term ID: {node_b}\")\n",
    "\n",
    "        # Step 1: Get parents for both nodes\n",
    "        parents_a = set(self.get_parents(node_a, include_self=False))\n",
    "        parents_b = set(self.get_parents(node_b, include_self=False))\n",
    "\n",
    "        # Step 2: Intersection of parents indicates sibling relationship\n",
    "        shared_parents = parents_a.intersection(parents_b)\n",
    "\n",
    "        # Step 3: Return True if they share any parent\n",
    "        return len(shared_parents) > 0\n",
    "    \n",
    "    def get_trajectories_from_root(self, term_id: str) -> list[list[dict]]:\n",
    "        \"\"\"\n",
    "        Get all ancestor trajectories from the root(s) to the given term using GraphBLAS operations.\n",
    "\n",
    "        Args:\n",
    "            term_id (str): The identifier of the term.\n",
    "\n",
    "        Returns:\n",
    "            list[list[dict]]: List of trajectories; each trajectory is a list of dictionaries\n",
    "                            with keys: 'id', 'name', and 'distance' (from the queried term).\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        lut_term_to_index = self.lookup_tables.get_lut_term_to_index()\n",
    "        if term_id not in lut_term_to_index:\n",
    "            raise KeyError(f\"Unknown term ID: {term_id}\")\n",
    "\n",
    "        A_T = self.matrices_container['is_a'].T\n",
    "        term_idx = int(self.lookup_tables.term_to_index(term_id))\n",
    "\n",
    "        # Root detection\n",
    "        roots = set(self.get_root())\n",
    "        root_indices = {int(self.lookup_tables.term_to_index(r)) for r in roots}\n",
    "\n",
    "        from collections import deque\n",
    "        queue = deque([[term_idx]])\n",
    "        trajectories = []\n",
    "\n",
    "        while queue:\n",
    "            path = queue.popleft()\n",
    "            current_idx = int(path[0])\n",
    "\n",
    "            # Parent discovery using GraphBLAS multiplication\n",
    "            parent_vec = (A_T @ self.one_hot_vector(current_idx)).new()\n",
    "            parent_indices = [int(i) for i in parent_vec.to_coo()[0]]\n",
    "\n",
    "            # Termination condition: reached a root or no parents\n",
    "            if not parent_indices or current_idx in root_indices:\n",
    "                # Reverse path → root → term order\n",
    "                reversed_path = list(reversed(path))\n",
    "                traj = []\n",
    "                for dist, idx in enumerate(reversed_path[::-1]):  # distance from term\n",
    "                    idx = int(idx)\n",
    "                    traj.append({\n",
    "                        'id': self.lookup_tables.index_to_term(idx),\n",
    "                        'name': self.lookup_tables.term_to_description(self.lookup_tables.index_to_term(idx)),\n",
    "                        'distance': dist\n",
    "                    })\n",
    "                trajectories.append(list(reversed(traj)))  # ensure root→term order\n",
    "            else:\n",
    "                for p in parent_indices:\n",
    "                    if p not in path:\n",
    "                        queue.append([p] + path)\n",
    "\n",
    "        for traj in trajectories:\n",
    "            traj.reverse()  # optional: reverse to have root-first order\n",
    "\n",
    "        return trajectories  # optional: reverse to have root-first order\n",
    "    \n",
    "    def print_term_trajectories_tree(self,trajectories: list[dict]) -> None:\n",
    "        \"\"\"Print all ancestor trajectories as a single ASCII tree from root to the original term.\n",
    "\n",
    "        Combining shared nodes.\n",
    "\n",
    "        Args:\n",
    "            trajectories: List of lists, each inner list is a trajectory (branch) as returned by ancestor_trajectories.\n",
    "        \"\"\"\n",
    "        if not trajectories:\n",
    "            print('No trajectories to display.')\n",
    "            return\n",
    "        root = self._build_tree_from_trajectories(trajectories)\n",
    "        self._print_ascii_tree(root)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_tree_from_trajectories(trajectories: list[dict]) -> object:\n",
    "        \"\"\"Build a tree structure from the list of branches (trajectories).\n",
    "\n",
    "        Returns the root node.\n",
    "\n",
    "        Args:\n",
    "            trajectories (list[dict]): List of trajectory branches.\n",
    "\n",
    "        Returns:\n",
    "            object: The root node of the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        class Node:\n",
    "            def __init__(self, node_id: str, name: str, distance: int) -> None:\n",
    "                self.id = node_id\n",
    "                self.name = name\n",
    "                self.distance = distance\n",
    "                self.children = {}\n",
    "\n",
    "        def insert_branch(root: Node, branch: list) -> None:\n",
    "            node = root\n",
    "            for item in branch:\n",
    "                key = (item['id'], item['name'], item['distance'])\n",
    "                if key not in node.children:\n",
    "                    node.children[key] = Node(*key)\n",
    "                node = node.children[key]\n",
    "\n",
    "        # All branches are sorted from term to root, so reverse to root-to-term\n",
    "        branch_lists = [list(branch) for branch in trajectories]\n",
    "        root_info = branch_lists[0][0]\n",
    "        root = Node(root_info['id'], root_info['name'], root_info['distance'])\n",
    "        for branch in branch_lists:\n",
    "            insert_branch(root, branch[1:])  # skip root itself, already created\n",
    "        return root\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_ascii_tree(root: object) -> None:\n",
    "        \"\"\"Print the tree structure in ASCII format starting from the root node.\"\"\"\n",
    "\n",
    "        def print_ascii_tree(\n",
    "            node: object, prefix: str = '', is_last: bool = True\n",
    "        ) -> None:\n",
    "            connector = '└── ' if is_last else '├── '\n",
    "            print(\n",
    "                f'{prefix}{connector}{node.id}: {node.name} (distance={node.distance})'\n",
    "            )\n",
    "            child_items = list(node.children.values())\n",
    "            for idx, child in enumerate(child_items):\n",
    "                is_last_child = idx == len(child_items) - 1\n",
    "                next_prefix = prefix + ('    ' if is_last else '│   ')\n",
    "                print_ascii_tree(child, next_prefix, is_last_child)\n",
    "\n",
    "        # Print root without prefix\n",
    "        print(f'{root.id}: {root.name} (distance={root.distance})')\n",
    "        child_items = list(root.children.values())\n",
    "        for idx, child in enumerate(child_items):\n",
    "            is_last_child = idx == len(child_items) - 1\n",
    "            print_ascii_tree(child, '', is_last_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Union\n",
    "import re\n",
    "\n",
    "class QueryEngine:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph  # Graph must contain lookup_tables\n",
    "\n",
    "    # --------------------------\n",
    "    # Tokenize\n",
    "    # --------------------------\n",
    "    @staticmethod\n",
    "    def parse_query(query: str) -> List[str]:\n",
    "        token_pattern = r\"'[^']+'|\\(|\\)|\\bAND\\b|\\bOR\\b|\\bNOT\\b|\\bDESCENDANTS\\b|\\bANCESTORS\\b\"\n",
    "        tokens = re.findall(token_pattern, query, flags=re.IGNORECASE)\n",
    "        return [t.upper() if t.upper() in {\"AND\", \"OR\", \"NOT\", \"DESCENDANTS\", \"ANCESTORS\"} else t.strip(\"'\") for t in tokens]\n",
    "\n",
    "    # --------------------------\n",
    "    # Infix → Postfix\n",
    "    # --------------------------\n",
    "    def _infix_to_postfix(self, tokens: List[str]) -> List[str]:\n",
    "        precedence = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}\n",
    "        output, stack = [], []\n",
    "        for token in tokens:\n",
    "            if token not in {\"AND\", \"OR\", \"NOT\", \"(\", \")\"}:\n",
    "                output.append(token)\n",
    "            elif token == \"(\":\n",
    "                stack.append(token)\n",
    "            elif token == \")\":\n",
    "                while stack and stack[-1] != \"(\":\n",
    "                    output.append(stack.pop())\n",
    "                stack.pop()\n",
    "            else:\n",
    "                while stack and stack[-1] != \"(\" and precedence.get(stack[-1], 0) >= precedence[token]:\n",
    "                    output.append(stack.pop())\n",
    "                stack.append(token)\n",
    "        while stack:\n",
    "            output.append(stack.pop())\n",
    "        return output\n",
    "\n",
    "    # --------------------------\n",
    "    # Set operations\n",
    "    # --------------------------\n",
    "    def _op_and(self, sets: List[Set[int]]) -> Set[int]:\n",
    "        if not sets:\n",
    "            return set()\n",
    "        res = sets[0]\n",
    "        for s in sets[1:]:\n",
    "            res &= s\n",
    "        return res\n",
    "\n",
    "    def _op_or(self, sets: List[Set[int]]) -> Set[int]:\n",
    "        res = set()\n",
    "        for s in sets:\n",
    "            res |= s\n",
    "        return res\n",
    "\n",
    "    def _op_not(self, universe: Set[int], s: Set[int]) -> Set[int]:\n",
    "        return universe - s\n",
    "\n",
    "    # --------------------------\n",
    "    # Resolve description or GO term → indexes\n",
    "    # --------------------------\n",
    "    def _resolve_token(self, token: str) -> Set[int]:\n",
    "        \"\"\"\n",
    "        Return set of indexes corresponding to the token.\n",
    "        Token can be:\n",
    "        - description (like 'actomyosin')\n",
    "        - GO term\n",
    "        \"\"\"\n",
    "        lut = self.graph.lookup_tables\n",
    "\n",
    "        # Try description first\n",
    "        try:\n",
    "            term = lut.description_to_term(token)\n",
    "        except KeyError:\n",
    "            term = token  # assume it is already a GO term\n",
    "\n",
    "        # Get descendants in the graph\n",
    "        try:\n",
    "            descendants = self.graph.get_descendants(term, include_self=True)\n",
    "        except KeyError:\n",
    "            descendants = []\n",
    "\n",
    "        # Map to indexes\n",
    "        indexes = set()\n",
    "        for d in descendants:\n",
    "            try:\n",
    "                idx = lut.term_to_index(d)\n",
    "                indexes.add(idx)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return indexes\n",
    "\n",
    "    # --------------------------\n",
    "    # Postfix evaluation\n",
    "    # --------------------------\n",
    "    def _eval_postfix(self, postfix: List[str]) -> Set[int]:\n",
    "        stack = []\n",
    "        universe = set(range(len(self.graph.lookup_tables.get_lut_index_to_term())))\n",
    "        for token in postfix:\n",
    "            if token in {\"AND\", \"OR\"}:\n",
    "                b, a = stack.pop(), stack.pop()\n",
    "                stack.append(self._op_and([a, b]) if token == \"AND\" else self._op_or([a, b]))\n",
    "            elif token == \"NOT\":\n",
    "                s = stack.pop()\n",
    "                stack.append(self._op_not(universe, s))\n",
    "            else:\n",
    "                stack.append(self._resolve_token(token))\n",
    "        if len(stack) != 1:\n",
    "            raise ValueError(\"Malformed query. Check parentheses and operators.\")\n",
    "        return stack[0]\n",
    "\n",
    "    # --------------------------\n",
    "    # Public API\n",
    "    # --------------------------\n",
    "    def execute_query(self, query: str) -> Set[int]:\n",
    "        tokens = self.parse_query(query)\n",
    "        postfix = self._infix_to_postfix(tokens)\n",
    "        return self._eval_postfix(postfix)\n",
    "\n",
    "    def format_results(self, idx_set: Set[int]) -> List[str]:\n",
    "        lut = self.graph.lookup_tables\n",
    "        return sorted([lut.index_to_term(i) for i in idx_set])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = LookUpTables(ontology=go_ontology)\n",
    "\n",
    "G = Graph(ontology=go_ontology, lookup_tables=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.print_term_trajectories_tree(G.get_trajectories_from_root('GO:0000017'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_siblings('GO:0000017', include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_sibling('GO:0015759', 'GO:0051325')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_path_between('GO:0000092', 'GO:0051325')\n",
    "L.term_to_description(G.get_path_between('GO:0044848', 'GO:0000092'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_distance_from_root('GO:0000092')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_common_ancestors(['GO:0000092', 'GO:0051325'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.edges_container['ends_during'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_during_list_indexes = []\n",
    "for k, v in G.edges_container['ends_during'].items():\n",
    "    print(f\"{k}: {v}\")\n",
    "    end_during_list_indexes.extend(v)\n",
    "\n",
    "L.term_to_description(L.index_to_term(end_during_list_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_children('GO:0048308', include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_parents('GO:0048308', include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_ancestors('GO:0051322', distance=5, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_ancestors_with_distance('GO:0051322', include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_descendants('GO:0051322', distance=5, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_descendants_with_distance('GO:0051322', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Human-readable query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List\n",
    "import re\n",
    "\n",
    "class QueryEngine:\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"\n",
    "        Initialize the QueryEngine with a Graph instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : Graph\n",
    "            Your ontology graph instance.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "\n",
    "    # --------------------------\n",
    "    # Parsing\n",
    "    # --------------------------\n",
    "    @staticmethod\n",
    "    def parse_query(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Very simple parser: splits query into tokens (terms, AND, OR, NOT, parentheses).\n",
    "\n",
    "        Examples:\n",
    "            \"'actomyosin' AND 'stress fiber'\"\n",
    "            \"(term1 OR term2) AND NOT term3\"\n",
    "        \"\"\"\n",
    "        # Match quoted terms, operators, and parentheses\n",
    "        token_pattern = r\"'[^']+'|\\(|\\)|\\bAND\\b|\\bOR\\b|\\bNOT\\b\"\n",
    "        tokens = re.findall(token_pattern, query, flags=re.IGNORECASE)\n",
    "        return [t.upper() if t.upper() in {\"AND\", \"OR\", \"NOT\"} else t.strip(\"'\") for t in tokens]\n",
    "\n",
    "    # --------------------------\n",
    "    # Postfix conversion (shunting-yard)\n",
    "    # --------------------------\n",
    "    def _infix_to_postfix(self, tokens: List[str]) -> List[str]:\n",
    "        precedence = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}\n",
    "        output = []\n",
    "        stack = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in {\"AND\", \"OR\", \"NOT\", \"(\", \")\"}:\n",
    "                output.append(token)\n",
    "            elif token == \"(\":\n",
    "                stack.append(token)\n",
    "            elif token == \")\":\n",
    "                while stack and stack[-1] != \"(\":\n",
    "                    output.append(stack.pop())\n",
    "                stack.pop()  # remove '('\n",
    "            else:\n",
    "                while stack and stack[-1] != \"(\" and precedence.get(stack[-1], 0) >= precedence[token]:\n",
    "                    output.append(stack.pop())\n",
    "                stack.append(token)\n",
    "\n",
    "        while stack:\n",
    "            output.append(stack.pop())\n",
    "\n",
    "        return output\n",
    "\n",
    "    # --------------------------\n",
    "    # Query operations\n",
    "    # --------------------------\n",
    "    def _operation_AND(self, sets: List[Set[str]]) -> Set[str]:\n",
    "        if not sets:\n",
    "            return set()\n",
    "        result_set = sets[0]\n",
    "        for s in sets[1:]:\n",
    "            result_set = result_set.intersection(s)\n",
    "        return result_set\n",
    "\n",
    "    def _operation_OR(self, sets: List[Set[str]]) -> Set[str]:\n",
    "        result_set = set()\n",
    "        for s in sets:\n",
    "            result_set.update(s)\n",
    "        return result_set\n",
    "\n",
    "    def _operation_NOT(self, base_set: Set[str], term_set: Set[str]) -> Set[str]:\n",
    "        return base_set - term_set\n",
    "\n",
    "    # --------------------------\n",
    "    # Postfix evaluation\n",
    "    # --------------------------\n",
    "    def _eval_postfix(self, postfix: List[str]) -> Set[str]:\n",
    "        stack = []\n",
    "        for token in postfix:\n",
    "            if token in {\"AND\", \"OR\"}:\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == \"AND\":\n",
    "                    stack.append(self._operation_AND([left, right]))\n",
    "                else:\n",
    "                    stack.append(self._operation_OR([left, right]))\n",
    "            elif token == \"NOT\":\n",
    "                operand = stack.pop()\n",
    "                # Universe = all nodes in the graph\n",
    "                universe = set(self.graph.nodes_container)\n",
    "                stack.append(self._operation_NOT(universe, operand))\n",
    "            else:\n",
    "                # Convert term to descendants set\n",
    "                descendants = set(self.graph.get_descendants(token, include_self=False))\n",
    "                stack.append(descendants)\n",
    "\n",
    "        if len(stack) != 1:\n",
    "            raise ValueError(\"Malformed query. Check parentheses and operators.\")\n",
    "        return stack[0]\n",
    "\n",
    "    # --------------------------\n",
    "    # Public API\n",
    "    # --------------------------\n",
    "    def execute_query(self, query: str) -> Set[str]:\n",
    "        tokens = self.parse_query(query)\n",
    "        postfix = self._infix_to_postfix(tokens)\n",
    "        result = self._eval_postfix(postfix)\n",
    "        return result\n",
    "\n",
    "    def format_results(self, result_set: Set[str]) -> List[str]:\n",
    "        return sorted(result_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.term_to_description(G.get_children(L.description_to_term('actomyosin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = QueryEngine(G)\n",
    "\n",
    "query1 = \"'striated muscle myosin thick filament assembly' AND 'cellular process'\"\n",
    "results1 = qe.execute_query(query1)\n",
    "print(qe.format_results(results1))\n",
    "print(L.term_to_description(qe.format_results(results1)))\n",
    "\n",
    "query2 = \"'striated muscle myosin thick filament assembly' OR 'cellular process'\"\n",
    "results2 = qe.execute_query(query2)\n",
    "print('\\n',qe.format_results(results2))\n",
    "print(L.term_to_description(qe.format_results(results2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "OPERATORS = {\"AND\", \"OR\", \"NOT\"}\n",
    "PARENS = {\"(\", \")\"}\n",
    "\n",
    "def tokenize_query(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a query with:\n",
    "      - GO IDs or single words\n",
    "      - Quoted phrases (single or double quotes)\n",
    "      - Logical operators: AND, OR, NOT\n",
    "      - Parentheses: (, )\n",
    "    \"\"\"\n",
    "    # Add spaces around parentheses to force separation\n",
    "    query = query.replace('(', ' ( ').replace(')', ' ) ')\n",
    "    \n",
    "    # Regex to match quoted phrases or operators/words\n",
    "    pattern = r'(\".*?\"|\\'.*?\\'|\\bAND\\b|\\bOR\\b|\\bNOT\\b|\\S+)'\n",
    "    tokens = re.findall(pattern, query, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove quotes from phrases\n",
    "    tokens = [t[1:-1] if (t.startswith(\"'\") or t.startswith('\"')) else t for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"GO:0008150 OR ('neuron apoptotic process' AND NOT membrane)\"\n",
    "tokens = tokenize_query(query)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### 2. Expression Tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExprNode:\n",
    "    def __init__(self, value, left=None, right=None):\n",
    "        \"\"\"\n",
    "        Expression tree node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : str\n",
    "            Operator ('AND', 'OR', 'NOT') or term (leaf node)\n",
    "        left : ExprNode or None\n",
    "            Left child (for binary operators)\n",
    "        right : ExprNode or None\n",
    "            Right child (for binary operators)\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_leaf():\n",
    "            return f\"Leaf({self.value})\"\n",
    "        elif self.value == \"NOT\":\n",
    "            return f\"NOT({self.left})\"\n",
    "        else:\n",
    "            return f\"({self.left} {self.value} {self.right})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECEDENCE = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}\n",
    "\n",
    "def parse_tokens(tokens: list[str]) -> ExprNode:\n",
    "    \"\"\"\n",
    "    Parse token list into a logical expression tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list[str]\n",
    "        Tokenized query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ExprNode\n",
    "        Root of the expression tree.\n",
    "    \"\"\"\n",
    "    node_stack = []\n",
    "    op_stack = []\n",
    "\n",
    "    def apply_operator():\n",
    "        op = op_stack.pop()\n",
    "        if op == \"NOT\":\n",
    "            operand = node_stack.pop()\n",
    "            node_stack.append(ExprNode(op, left=operand))\n",
    "        else:\n",
    "            right = node_stack.pop()\n",
    "            left = node_stack.pop()\n",
    "            node_stack.append(ExprNode(op, left=left, right=right))\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in PRECEDENCE:\n",
    "            while (\n",
    "                op_stack\n",
    "                and op_stack[-1] in PRECEDENCE\n",
    "                and PRECEDENCE[op_stack[-1]] >= PRECEDENCE[token]\n",
    "            ):\n",
    "                apply_operator()\n",
    "            op_stack.append(token)\n",
    "        elif token == \"(\":\n",
    "            op_stack.append(token)\n",
    "        elif token == \")\":\n",
    "            while op_stack and op_stack[-1] != \"(\":\n",
    "                apply_operator()\n",
    "            if not op_stack:\n",
    "                raise ValueError(\"Mismatched parentheses\")\n",
    "            op_stack.pop()  # remove \"(\"\n",
    "        else:\n",
    "            node_stack.append(ExprNode(token))\n",
    "\n",
    "    while op_stack:\n",
    "        if op_stack[-1] in (\"(\", \")\"):\n",
    "            raise ValueError(\"Mismatched parentheses\")\n",
    "        apply_operator()\n",
    "\n",
    "    if len(node_stack) != 1:\n",
    "        raise ValueError(\"Invalid expression\")\n",
    "    return node_stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_query(query)\n",
    "tree = parse_tokens(tokens)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 3. Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree(node, graph, leaf_strategy=\"descendants\"):\n",
    "    \"\"\"\n",
    "    Recursively evaluate a logical expression tree of ontology terms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : TreeNode\n",
    "        Node of the expression tree (operator or leaf).\n",
    "    graph : Graph\n",
    "        Graph object containing ontology matrices and lookup tables.\n",
    "    leaf_strategy : str\n",
    "        How to resolve leaves: 'descendants', 'ancestors', or 'self'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Set[str]\n",
    "        Set of GO term IDs matching the query.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Base case: leaf node ---\n",
    "    if node.is_leaf():\n",
    "        leaf_value = node.value\n",
    "\n",
    "        # Convert description or GO ID to term ID(s)\n",
    "        try:\n",
    "            term_ids = graph.lookup_tables.description_to_term(leaf_value)\n",
    "        except KeyError:\n",
    "            # Leaf not found → empty set\n",
    "            return set()\n",
    "\n",
    "        # Ensure list\n",
    "        if not isinstance(term_ids, list):\n",
    "            term_ids = [term_ids]\n",
    "\n",
    "        # Resolve leaf using Graph functions\n",
    "        result_set = set()\n",
    "        for tid in term_ids:\n",
    "            if leaf_strategy == \"descendants\":\n",
    "                result_set.update(graph.get_descendants(tid, include_self=False))\n",
    "            elif leaf_strategy == \"ancestors\":\n",
    "                result_set.update(graph.get_ancestors(tid, include_self=False))\n",
    "            elif leaf_strategy == \"self\":\n",
    "                result_set.add(tid)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown leaf_strategy: {leaf_strategy}\")\n",
    "\n",
    "        return result_set\n",
    "\n",
    "    # --- Recursive case: operator node ---\n",
    "    op = node.value.upper()\n",
    "\n",
    "    if op == \"NOT\":\n",
    "        child_result = evaluate_tree(node.left, graph, leaf_strategy)\n",
    "        # Universe is all nodes in the ontology\n",
    "        universe = set(graph.lookup_tables.get_lut_index_to_term())\n",
    "        return universe - child_result\n",
    "    elif op == \"AND\":\n",
    "        left_result = evaluate_tree(node.left, graph, leaf_strategy)\n",
    "        right_result = evaluate_tree(node.right, graph, leaf_strategy)\n",
    "        return left_result & right_result\n",
    "    elif op == \"OR\":\n",
    "        left_result = evaluate_tree(node.left, graph, leaf_strategy)\n",
    "        right_result = evaluate_tree(node.right, graph, leaf_strategy)\n",
    "        return left_result | right_result\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown operator: {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"'neuron apoptotic process'\"\n",
    "tokens = tokenize_query(query)          # Tokenizer from previous step\n",
    "tree = parse_tokens(tokens)             # Parser from previous step\n",
    "\n",
    "# Assume you already have a Graph object: `graph`\n",
    "result_set = list(evaluate_tree(tree, G, leaf_strategy=\"descendants\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.term_to_description(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ontograph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
