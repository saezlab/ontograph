{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pronto\n",
    "from pronto import LiteralPropertyValue, Xref\n",
    "from pronto import Synonym, SynonymData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Load Efficiently the dataset\n",
    "\n",
    "Requirements:\n",
    "- Tabular data in CSV, TSV format.\n",
    "- A YAML file with the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET= \"../data/in/lipids.tsv.gz\"\n",
    "#PATH_DATASET= \"../data/in/lipids_dummy.tsv\"\n",
    "PATH_CONFIG_LOADING = Path(\"./swisslipids_L.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## INGESTION STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_global_settings(filepath_configuration_file):\n",
    "    with open(filepath_configuration_file, 'r') as f:\n",
    "        global_settings = yaml.safe_load(f)\n",
    "    return global_settings\n",
    "\n",
    "def load_tabular_data(filepath, config_loading:dict=None):\n",
    "    \n",
    "    encoding = config_loading.get('encoding', 'utf-8')\n",
    "    separator = config_loading.get('delimiter', '\\t')\n",
    "    dtypes = config_loading.get('schema')\n",
    "    chunksize = config_loading.get('chunksize')\n",
    "\n",
    "    chunk_iterator = pd.read_table(\n",
    "        filepath_or_buffer=filepath,\n",
    "        encoding=encoding,\n",
    "        sep=separator,\n",
    "        dtype=dtypes,\n",
    "        chunksize=chunksize\n",
    "    )\n",
    "\n",
    "    return chunk_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file\n",
    "config_load = read_global_settings(filepath_configuration_file=PATH_CONFIG_LOADING)\n",
    "\n",
    "INGESTION_PARAMS = config_load.get(\"ingestion\")\n",
    "TRANSFORMATION_PARAMS = config_load.get(\"transformation\")\n",
    "SERVING_PARAMS = config_load.get(\"serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_lipids = load_tabular_data(\n",
    "    filepath=PATH_DATASET,\n",
    "    config_loading=INGESTION_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_special_characters(dataset_column):\n",
    "    unique_values = dataset_column.dropna()\n",
    "\n",
    "    special_chars = set()\n",
    "    for val in unique_values:\n",
    "        special_chars.update(re.findall(r'[^a-zA-Z0-9]', str(val)))\n",
    "\n",
    "    return special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Column:** `Lipid ID`\n",
    "\n",
    "- Show the special characters in this column\n",
    "- Number of unique elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = find_special_characters(df_lipids[\"Lipid ID\"])\n",
    "print(special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique elements: {df_lipids['Lipid ID'].nunique(dropna=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Column:** `Level`\n",
    "\n",
    "- Count the distinct values (categories) in this column\n",
    "- Display those row without an assigned category\n",
    "- Show the especial characters in this columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids[\"Level\"].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids[df_lipids[\"Level\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = find_special_characters(df_lipids[\"Level\"])\n",
    "print(special_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Column:** `Lipid class *`\n",
    "\n",
    "- Show the especial characters in this columns\n",
    "- Show some examples of rows containing the suspected separator character (for instance, pipes)\n",
    "- Count the number of unique classes\n",
    "- Count the number of IDs that belong to `Lipid Class*` and are not present in `Lipid ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = find_special_characters(df_lipids[\"Lipid class*\"])\n",
    "print(special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids[df_lipids[\"Lipid class*\"].str.contains(r'\\|', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "- `:` : this character is used as part of the Swiss Lipids identifiers i.e., `SLM:000389698`.\n",
    "- `|` : this character is used to separate elements of lists i.e., `SLM:000389698 | SLM:000399707`.\n",
    "- ` ` : the space character is used for human readability, they should be removed when processing individual cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = (\n",
    "    df_lipids[\"Lipid class*\"]\n",
    "    .dropna()\n",
    "    .str.split('|')\n",
    "    .explode()\n",
    "    .str.strip()\n",
    "    .loc[lambda x: x != '']\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "unique_classes = set(elements)\n",
    "\n",
    "print(f\"Unique classes in the column: {len(unique_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lipid_ids = set(df_lipids[\"Lipid ID\"].dropna().astype(str).str.strip().unique())\n",
    "for item in unique_classes:\n",
    "    if item.strip() not in lipid_ids:\n",
    "        print(f\"{item} not in LIPID ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**Column:** `Parent`\n",
    "\n",
    "- Show the especial characters in this\n",
    "- Count the number of unique classes\n",
    "- Count the number of IDs that belong to this columns and are not present in `Lipid ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = find_special_characters(df_lipids[\"Parent\"])\n",
    "print(special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = (\n",
    "    df_lipids[\"Parent\"]\n",
    "    .dropna()\n",
    "    .str.strip()\n",
    "    .loc[lambda x: x != '']\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "unique_classes = set(elements)\n",
    "\n",
    "print(f\"Unique Parents in the column: {len(unique_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lipid_ids = set(df_lipids[\"Lipid ID\"].dropna().astype(str).unique())\n",
    "parent_ids = set(df_lipids[\"Parent\"].dropna().astype(str).unique())\n",
    "unique_parents_not_in_lipid_ids = parent_ids - lipid_ids\n",
    "print(f\"Unique parents not in Lipid id: {len(unique_parents_not_in_lipid_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## TRANSFORMATION STAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Generate Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataset, dict_columns):\n",
    "    # Rename Columns\n",
    "    dataset.columns = dataset.columns.str.strip()\n",
    "    dataset.rename(columns=dict_columns, inplace=True)\n",
    "    print(dataset.columns)\n",
    "\n",
    "def strip_columns(dataset):\n",
    "    obj_cols = dataset.select_dtypes(include=\"string\").columns\n",
    "    dataset[obj_cols] = dataset[obj_cols].apply(lambda col: col.str.strip())\n",
    "\n",
    "def add_prefixes(dataset: pd.DataFrame, dict_prefixes: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds prefixes to specified columns in a DataFrame if they are not already present.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The input DataFrame.\n",
    "        dict_prefixes (dict): A dictionary mapping column names to their prefixes.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with prefixes applied where needed.\n",
    "    \"\"\"\n",
    "    for column, prefix in dict_prefixes.items():\n",
    "        if column in dataset.columns:\n",
    "            # Ensure the column is treated as a string for the check\n",
    "            col_str = dataset[column].astype(str)\n",
    "            \n",
    "            # Create a mask for non-null values that do not already start with the prefix\n",
    "            mask = dataset[column].notna() & ~col_str.str.startswith(prefix, na=False)\n",
    "            \n",
    "            # Apply the prefix only to the selected rows\n",
    "            dataset.loc[mask, column] = prefix + dataset.loc[mask, column].astype(str)\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "def create_ontology():\n",
    "    ontology = pronto.Ontology()\n",
    "    return ontology\n",
    "\n",
    "def add_ontology_metadata(ontology, metadata):\n",
    "    for key, value in metadata.items():\n",
    "        setattr(ontology.metadata, key, value)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "\n",
    "    # Rename Columns\n",
    "    rename_columns(dataset, MAPPING_COLUMN_NAMES)\n",
    "\n",
    "    # Strip values in columns (vectorized)\n",
    "    strip_columns(dataset)\n",
    "\n",
    "    # Add prefixes in certain columns\n",
    "    dataset = add_prefixes(dataset=dataset, dict_prefixes=PREFIXES_MAPPING_IDS)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def extract_all_ids(dataset, mapping_column_ids):\n",
    "    \n",
    "    superset_unique_ids = set()\n",
    "    print(mapping_column_ids)\n",
    "\n",
    "    for k, v in mapping_column_ids.items():\n",
    "        column_name = v[\"name\"]\n",
    "        separator = v[\"separator\"]\n",
    "\n",
    "        if separator is None:\n",
    "            #print(k)\n",
    "            temporal_ids = dataset[column_name].dropna()\n",
    "        else:\n",
    "            temporal_series = dataset[column_name].dropna().str.split(separator).explode()\n",
    "            temporal_ids = temporal_series.str.strip().loc[lambda x: x != ''].unique()\n",
    "\n",
    "        superset_unique_ids.update(set(temporal_ids))\n",
    "\n",
    "    print(f\"Total unique terms to create: {len(superset_unique_ids)}\")\n",
    "    return superset_unique_ids\n",
    "    \n",
    "def create_all_terms(ontology, set_of_ids):\n",
    "    terms_dict = {term_id: ontology.create_term(term_id) for term_id in set_of_ids}\n",
    "\n",
    "    return terms_dict\n",
    "\n",
    "def add_properties(dataset, map_properties, terms_dict, column_id):\n",
    "        # # 3.1 Add all properties (name, synonym, annotation, xref)\n",
    "        print(f\"TRACK: {column_id}\")\n",
    "        for row in dataset.itertuples(index=False):\n",
    "            term_id = getattr(row, column_id)\n",
    "            \n",
    "            # Add name property\n",
    "            terms_dict[term_id].name = getattr(row, map_properties[\"name\"])\n",
    "            \n",
    "            # Add annotations properties\n",
    "            for column_name, datatype in ALL_PROPERTIES[\"annotation\"].items():\n",
    "                prop = getattr(row, column_name)\n",
    "                if pd.notna(prop):\n",
    "                    literal_value = LiteralPropertyValue(column_name, str(prop), datatype=datatype[\"datatype\"])\n",
    "                    terms_dict[term_id].annotations.add(literal_value)\n",
    "\n",
    "            # Add references\n",
    "            for reference in map_properties[\"references\"]:\n",
    "                ref = getattr(row, reference)\n",
    "                if pd.notna(ref):\n",
    "                    literal_value = LiteralPropertyValue(reference, str(ref), datatype=\"xsd:string\")\n",
    "                    terms_dict[term_id].annotations.add(literal_value)\n",
    "\n",
    "def add_parent_relationships(term_definitions, id_columns, terms_dict):\n",
    "    \"\"\"Adds 'is_a' relationships from terms to their parents.\"\"\"\n",
    "    parent_col_name = id_columns[\"parent_id\"][\"name\"]\n",
    "    parent_series = term_definitions[parent_col_name].dropna()\n",
    "    for child_id, parent_id in parent_series.items():\n",
    "        child_term = terms_dict.get(child_id)\n",
    "        parent_term = terms_dict.get(parent_id)\n",
    "        if child_term and parent_term:\n",
    "            child_term.superclasses().add(parent_term)\n",
    "\n",
    "def add_class_relationships(term_definitions, id_columns, terms_dict):\n",
    "    \"\"\"Adds 'is_a' relationships from terms to their classes.\"\"\"\n",
    "    class_col_name = id_columns[\"class_id\"][\"name\"]\n",
    "    class_separator = id_columns[\"class_id\"][\"separator\"]\n",
    "    class_series = term_definitions[class_col_name].dropna()\n",
    "    for term_id, classes_str in class_series.items():\n",
    "        term = terms_dict.get(term_id)\n",
    "        if not term:\n",
    "            continue\n",
    "        for class_id in classes_str.split(class_separator):\n",
    "            class_term = terms_dict.get(class_id.strip())\n",
    "            if class_term:\n",
    "                term.superclasses().add(class_term)\n",
    "\n",
    "def generate_ontology_from_table(dataset: pd.DataFrame, id_columns: dict, metadata_ontology:dict):\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    dataset = preprocess_dataset(dataset)\n",
    "\n",
    "    # Create Ontology\n",
    "    ontology = create_ontology()\n",
    "    \n",
    "    # Add ontology metadata\n",
    "    add_ontology_metadata(ontology=ontology, metadata=metadata_ontology)\n",
    "\n",
    "    # --- 1. Collect all unique IDs from all relevant columns ---\n",
    "    mapping_ids = config_load[\"transformation\"][\"generate_ontology\"][\"columns_for_terms\"]\n",
    "    all_unique_ids = extract_all_ids(dataset, mapping_ids)\n",
    "\n",
    "\n",
    "    # # --- 2. Create all terms ONCE and store them in a dictionary for fast access ---\n",
    "    # terms_dict = {term_id: ontology.create_term(term_id) for term_id in all_unique_ids}\n",
    "    terms_dict = create_all_terms(ontology=ontology, set_of_ids=all_unique_ids)\n",
    "\n",
    "    # # --- 3. Add properties and relationships using fast, column-based operations ---\n",
    "\n",
    "    # # Process only the rows that define a term (non-null term_id)\n",
    "    column_terms = id_columns[\"term_id\"][\"name\"]\n",
    "    print(column_terms)\n",
    "    term_definitions = dataset.dropna(subset=[column_terms]).set_index(column_terms)\n",
    "\n",
    "    # 3.1 Add properties\n",
    "    add_properties(dataset=dataset,\n",
    "                   map_properties=ALL_PROPERTIES,\n",
    "                   terms_dict=terms_dict,\n",
    "                   column_id=column_terms\n",
    "    )\n",
    "    \n",
    "                    \n",
    "\n",
    "    # 3.2 Add Parent relationships (is_a) - NO .iterrows()\n",
    "    add_parent_relationships(term_definitions=term_definitions,\n",
    "                             id_columns=id_columns,\n",
    "                             terms_dict=terms_dict\n",
    "    )\n",
    "\n",
    "    # 3.3 Add Class relationships (is_a) - NO .iterrows()\n",
    "    add_class_relationships(term_definitions=term_definitions,\n",
    "                             id_columns=id_columns,\n",
    "                             terms_dict=terms_dict\n",
    "    )\n",
    "    \n",
    "    return ontology\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING_COLUMN_NAMES = config_load[\"transformation\"][\"rename_columns\"]\n",
    "\n",
    "METADATA_ONTOLOGY = config_load[\"transformation\"][\"generate_ontology\"][\"ontology_metadata\"]\n",
    "\n",
    "COLUMNS_FOR_RELATIONSHIPS = config_load[\"transformation\"][\"generate_ontology\"][\"columns_for_terms\"]\n",
    "\n",
    "ALL_PROPERTIES = config_load[\"transformation\"][\"generate_ontology\"][\"properties\"]\n",
    "\n",
    "PREFIXES_MAPPING_IDS = config_load[\"transformation\"][\"add_prefixes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lipids = preprocess_dataset(df_lipids)\n",
    "df_lipids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "swissontology = generate_ontology_from_table(dataset=df_lipids,\n",
    "                                             id_columns=COLUMNS_FOR_RELATIONSHIPS,\n",
    "                                             metadata_ontology=METADATA_ONTOLOGY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(swissontology.terms())) # 779260"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## SERVING STAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Export Ontology file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ontology(file_path, ontology, ontology_serializer:str=\"obo\"):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        ontology.dump(f, format=ontology_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_ONTOLOGY_FILE = SERVING_PARAMS[\"ontology\"]\n",
    "\n",
    "export_ontology(file_path=PARAMS_ONTOLOGY_FILE[\"path_file\"],\n",
    "                ontology=swissontology,\n",
    "                ontology_serializer=PARAMS_ONTOLOGY_FILE[\"serializer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Export Mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mapping_file(filepath, format, delimiter, dataset, list_columns):\n",
    "    # Select dataset columns\n",
    "    dataframe = dataset[list_columns]\n",
    "\n",
    "    # Store dataset as CSV\n",
    "    dataframe.to_csv(filepath+\".\"+format, sep=delimiter, index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_MAPPING_FILE = SERVING_PARAMS[\"mapping_file\"]\n",
    "\n",
    "generate_mapping_file(filepath=PARAMS_MAPPING_FILE[\"path_file\"],\n",
    "                      format=PARAMS_MAPPING_FILE[\"format\"],\n",
    "                      delimiter=PARAMS_MAPPING_FILE[\"delimiter\"],\n",
    "                      dataset=df_lipids,\n",
    "                      list_columns=TRANSFORMATION_PARAMS[\"generate_mapping_file\"][\"columns\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ontograph (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
