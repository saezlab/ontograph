{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronto\n",
    "import graphblas as gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Step 1. Load an ontology using Pronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_ontology = pronto.Ontology('../data/out/go.obo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chebi = pronto.Ontology('../data/out/chebi.obo', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Step 2. Create Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Create Nodes Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a string by multiple separators and get the last part\n",
    "def get_last_part_string(s, separators=('/', ':', '.', '#')):\n",
    "    # Create a regex pattern to split by any of the separators\n",
    "    pattern = '|'.join(map(re.escape, separators))\n",
    "    parts = re.split(pattern, s)\n",
    "\n",
    "    return parts[-1] if parts else s\n",
    "\n",
    "\n",
    "def get_dictitionary_annotations(annotations):\n",
    "    ann_dict = {}\n",
    "    for annotation in annotations:\n",
    "        # Get \"annotation.property\"\n",
    "        key = get_last_part_string(annotation.property)\n",
    "\n",
    "        # Get elements from ResourcePropertyValue annotations\n",
    "        if isinstance(annotation, pronto.ResourcePropertyValue):\n",
    "            ann_dict[key] = {'resource': annotation.resource}\n",
    "            continue\n",
    "\n",
    "        # Get elements from LiteralPropertyValue annotations\n",
    "        elif isinstance(annotation, pronto.LiteralPropertyValue):\n",
    "            ann_dict[key] = {\n",
    "                'literal': annotation.literal,\n",
    "                'datatype': get_last_part_string(annotation.datatype),\n",
    "            }\n",
    "    return ann_dict\n",
    "\n",
    "\n",
    "def get_string_relationships(relations):\n",
    "    rel_list = [relation.name for relation in relations.keys()]\n",
    "    return ('|').join(rel_list)\n",
    "\n",
    "\n",
    "def get_dictionary_synonyms(synonyms):\n",
    "    syn_dict = {}\n",
    "    for synonym in synonyms:\n",
    "        entry = {\n",
    "            k: v\n",
    "            for k, v in [\n",
    "                (\n",
    "                    'type',\n",
    "                    getattr(synonym.type, 'id', None) if synonym.type else None,\n",
    "                ),\n",
    "                (\n",
    "                    'source',\n",
    "                    '|'.join(str(source.id) for source in synonym.xrefs)\n",
    "                    if synonym.xrefs\n",
    "                    else None,\n",
    "                ),\n",
    "                ('scope', synonym.scope if synonym.scope is not None else None),\n",
    "            ]\n",
    "            if v is not None and v != ''\n",
    "        }\n",
    "        syn_dict[synonym.description] = entry\n",
    "    return syn_dict\n",
    "\n",
    "\n",
    "def get_dictionary_xrefs(xrefs):\n",
    "    xref_dict = {}\n",
    "    for xref in xrefs:\n",
    "        entry = {\n",
    "            k: v\n",
    "            for k, v in [\n",
    "                ('description', xref.description if xref.description else None)\n",
    "            ]\n",
    "            if v is not None and v != ''\n",
    "        }\n",
    "        xref_dict[f'{xref.id}'] = entry\n",
    "    return xref_dict\n",
    "\n",
    "\n",
    "def create_nodes_dataframe(terms, include_obsolete=False):\n",
    "    \"\"\"Create a DataFrame with fields: ID, Name, Definition, Namespace, Subsets, Synonyms, Xrefs.\"\"\"\n",
    "    # Pre-bind functions for efficiency\n",
    "    join = '|'.join\n",
    "    str_ = str\n",
    "    get_ann = get_dictitionary_annotations\n",
    "    get_syn = get_dictionary_synonyms\n",
    "    get_xref = get_dictionary_xrefs\n",
    "    get_rel = get_string_relationships\n",
    "\n",
    "    rows = [\n",
    "        {\n",
    "            # Identity & Naming\n",
    "            'term_id': term.id,\n",
    "            'name': term.name,\n",
    "            'alternate_ids': join(term.alternate_ids)\n",
    "            if term.alternate_ids\n",
    "            else None,\n",
    "            'namespace': term.namespace,\n",
    "            # Status & Lifecycle\n",
    "            'obsolete': term.obsolete,\n",
    "            'anonymous': term.anonymous,\n",
    "            'builtin': term.builtin,\n",
    "            'created_by': term.created_by,\n",
    "            'creation_date': term.creation_date,\n",
    "            'replaced_by': join([replacer.id for replacer in term.replaced_by])\n",
    "            if term.replaced_by\n",
    "            else None,\n",
    "            'consider': join(term.consider) if term.consider else None,\n",
    "            # Description & Annotation\n",
    "            'definition': str_(term.definition) if term.definition else None,\n",
    "            'comment': term.comment,\n",
    "            'annotations': str_(get_ann(term.annotations))\n",
    "            if term.annotations\n",
    "            else None,\n",
    "            'subsets': join(term.subsets) if term.subsets else None,\n",
    "            'synonyms': str_(get_syn(term.synonyms)) if term.synonyms else None,\n",
    "            'xrefs': str_(get_xref(term.xrefs)) if term.xrefs else None,\n",
    "            # Logical & Semantic Relations\n",
    "            'relationships': get_rel(term.relationships)\n",
    "            if term.relationships\n",
    "            else None,\n",
    "            'disjoint_from': term.disjoint_from if term.disjoint_from else None,\n",
    "            'equivalent_to': term.equivalent_to if term.equivalent_to else None,\n",
    "            'intersection_of': term.intersection_of\n",
    "            if term.intersection_of\n",
    "            else None,\n",
    "        }\n",
    "        for term in terms\n",
    "        if include_obsolete or not term.obsolete\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Sort by term_id and reset index\n",
    "    df.sort_values('term_id', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add index column\n",
    "    df.insert(0, 'index', range(len(df)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_nodes_dataframe(go_ontology.terms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Create Classes for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Look-up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Refactored LookUpTables: does NOT store terms ---\n",
    "class LookUpTables:\n",
    "    def __init__(self, terms: list):\n",
    "        self.__lut_term_to_index = {\n",
    "            term.id: idx for idx, term in enumerate(terms)\n",
    "        }\n",
    "        self.__lut_index_to_term = [term.id for term in terms]\n",
    "        self.__lut_term_to_description = {term.id: term.name for term in terms}\n",
    "        self.__lut_description_to_term = {term.name: term.id for term in terms}\n",
    "\n",
    "    def get_lut_term_to_index(self):\n",
    "        return self.__lut_term_to_index\n",
    "\n",
    "    def get_lut_index_to_term(self):\n",
    "        return self.__lut_index_to_term\n",
    "\n",
    "    def get_lut_term_to_description(self):\n",
    "        return self.__lut_term_to_description\n",
    "\n",
    "    def get_lut_description_to_term(self):\n",
    "        return self.__lut_description_to_term\n",
    "\n",
    "    def term_to_index(self, terms: str | list):\n",
    "        if isinstance(terms, str):\n",
    "            return self.__lut_term_to_index[terms]\n",
    "        elif isinstance(terms, list):\n",
    "            return [self.__lut_term_to_index[term] for term in terms]\n",
    "\n",
    "    def index_to_term(self, indexes: int | list):\n",
    "        if isinstance(indexes, int):\n",
    "            return self.__lut_index_to_term[indexes]\n",
    "        elif isinstance(indexes, list):\n",
    "            return [self.__lut_index_to_term[idx] for idx in indexes]\n",
    "        elif isinstance(indexes, np.ndarray):\n",
    "            return [self.__lut_index_to_term[idx] for idx in indexes.tolist()]\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'Expected int, list[int], or np.ndarray, got {type(indexes).__name__}.'\n",
    "            )\n",
    "\n",
    "    def term_to_description(self, terms: str | list):\n",
    "        if isinstance(terms, str):\n",
    "            return self.__lut_term_to_description[terms]\n",
    "        elif isinstance(terms, list):\n",
    "            return [self.__lut_term_to_description[term] for term in terms]\n",
    "\n",
    "    def description_to_term(self, descriptions: str | list):\n",
    "        if isinstance(descriptions, str):\n",
    "            return self.__lut_description_to_term[descriptions]\n",
    "        elif isinstance(descriptions, list):\n",
    "            return [\n",
    "                self.__lut_description_to_term[term] for term in descriptions\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Nodes [indexes and data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NodeContainer:\n",
    "    nodes_indices: np.ndarray\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nodes_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.nodes_indices[idx]\n",
    "\n",
    "    def as_list(self):\n",
    "        return self.nodes_indices.tolist()\n",
    "\n",
    "    def as_set(self):\n",
    "        return set(self.nodes_indices)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.nodes_indices\n",
    "\n",
    "\n",
    "class NodesDataframe:\n",
    "    def __init__(self, terms, include_obsolete=False):\n",
    "        self.dataframe = self.create_nodes_dataframe(\n",
    "            terms, include_obsolete=include_obsolete\n",
    "        )\n",
    "\n",
    "    # Function to split a string by multiple separators and get the last part\n",
    "    def __get_last_part_string(self, s, separators=('/', ':', '.', '#')):\n",
    "        # Create a regex pattern to split by any of the separators\n",
    "        pattern = '|'.join(map(re.escape, separators))\n",
    "        parts = re.split(pattern, s)\n",
    "\n",
    "        return parts[-1] if parts else s\n",
    "\n",
    "    def __get_dictitionary_annotations(self, annotations):\n",
    "        ann_dict = {}\n",
    "        for annotation in annotations:\n",
    "            # Get \"annotation.property\"\n",
    "            key = self.__get_last_part_string(annotation.property)\n",
    "\n",
    "            # Get elements from ResourcePropertyValue annotations\n",
    "            if isinstance(annotation, pronto.ResourcePropertyValue):\n",
    "                ann_dict[key] = {'resource': annotation.resource}\n",
    "                continue\n",
    "\n",
    "            # Get elements from LiteralPropertyValue annotations\n",
    "            elif isinstance(annotation, pronto.LiteralPropertyValue):\n",
    "                ann_dict[key] = {\n",
    "                    'literal': annotation.literal,\n",
    "                    'datatype': self.__get_last_part_string(\n",
    "                        annotation.datatype\n",
    "                    ),\n",
    "                }\n",
    "        return ann_dict\n",
    "\n",
    "    def __get_string_relationships(self, relations):\n",
    "        rel_list = [relation.name for relation in relations.keys()]\n",
    "        return ('|').join(rel_list)\n",
    "\n",
    "    def __get_dictionary_synonyms(self, synonyms):\n",
    "        syn_dict = {}\n",
    "        for synonym in synonyms:\n",
    "            entry = {\n",
    "                k: v\n",
    "                for k, v in [\n",
    "                    (\n",
    "                        'type',\n",
    "                        getattr(synonym.type, 'id', None)\n",
    "                        if synonym.type\n",
    "                        else None,\n",
    "                    ),\n",
    "                    (\n",
    "                        'source',\n",
    "                        '|'.join(str(source.id) for source in synonym.xrefs)\n",
    "                        if synonym.xrefs\n",
    "                        else None,\n",
    "                    ),\n",
    "                    (\n",
    "                        'scope',\n",
    "                        synonym.scope if synonym.scope is not None else None,\n",
    "                    ),\n",
    "                ]\n",
    "                if v is not None and v != ''\n",
    "            }\n",
    "            syn_dict[synonym.description] = entry\n",
    "        return syn_dict\n",
    "\n",
    "    def __get_dictionary_xrefs(self, xrefs):\n",
    "        xref_dict = {}\n",
    "        for xref in xrefs:\n",
    "            entry = {\n",
    "                k: v\n",
    "                for k, v in [\n",
    "                    (\n",
    "                        'description',\n",
    "                        xref.description if xref.description else None,\n",
    "                    )\n",
    "                ]\n",
    "                if v is not None and v != ''\n",
    "            }\n",
    "            xref_dict[f'{xref.id}'] = entry\n",
    "        return xref_dict\n",
    "\n",
    "    def create_nodes_dataframe(self, terms, include_obsolete=False):\n",
    "        \"\"\"Create a DataFrame with fields: ID, Name, Definition, Namespace, Subsets, Synonyms, Xrefs.\"\"\"\n",
    "        # Pre-bind functions for efficiency\n",
    "        join = '|'.join\n",
    "        str_ = str\n",
    "        get_ann = self.__get_dictitionary_annotations\n",
    "        get_syn = self.__get_dictionary_synonyms\n",
    "        get_xref = self.__get_dictionary_xrefs\n",
    "        get_rel = self.__get_string_relationships\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                # Identity & Naming\n",
    "                'term_id': term.id,\n",
    "                'name': term.name,\n",
    "                'alternate_ids': join(term.alternate_ids)\n",
    "                if term.alternate_ids\n",
    "                else None,\n",
    "                'namespace': term.namespace,\n",
    "                # Status & Lifecycle\n",
    "                'obsolete': term.obsolete,\n",
    "                'anonymous': term.anonymous,\n",
    "                'builtin': term.builtin,\n",
    "                'created_by': term.created_by,\n",
    "                'creation_date': term.creation_date,\n",
    "                'replaced_by': join(\n",
    "                    [replacer.id for replacer in term.replaced_by]\n",
    "                )\n",
    "                if term.replaced_by\n",
    "                else None,\n",
    "                'consider': join(term.consider) if term.consider else None,\n",
    "                # Description & Annotation\n",
    "                'definition': str(term.definition) if term.definition else None,\n",
    "                'comment': term.comment,\n",
    "                'annotations': str(get_ann(term.annotations))\n",
    "                if term.annotations\n",
    "                else None,\n",
    "                'subsets': join(term.subsets) if term.subsets else None,\n",
    "                'synonyms': str(get_syn(term.synonyms))\n",
    "                if term.synonyms\n",
    "                else None,\n",
    "                'xrefs': str(get_xref(term.xrefs)) if term.xrefs else None,\n",
    "                # Logical & Semantic Relations\n",
    "                'relationships': get_rel(term.relationships)\n",
    "                if term.relationships\n",
    "                else None,\n",
    "                'disjoint_from': term.disjoint_from\n",
    "                if term.disjoint_from\n",
    "                else None,\n",
    "                'equivalent_to': term.equivalent_to\n",
    "                if term.equivalent_to\n",
    "                else None,\n",
    "                'intersection_of': term.intersection_of\n",
    "                if term.intersection_of\n",
    "                else None,\n",
    "            }\n",
    "            for term in terms\n",
    "            if include_obsolete or not term.obsolete\n",
    "        ]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        # Sort by term_id and reset index\n",
    "        df.sort_values('term_id', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Add index column\n",
    "        df.insert(0, 'index', range(len(df)))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "term = go_ontology.get_term('GO:0008150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in go_ontology.relationships():\n",
    "    print(f'alternate_ids: {rel.alternate_ids}')\n",
    "    print(f'annotations: {rel.annotations}')\n",
    "    print(f'anonymous: {rel.anonymous}')\n",
    "    print(f'builtin: {rel.builtin}')\n",
    "    print(f'comment: {rel.comment}')\n",
    "    print(f'consider: {rel.consider}')\n",
    "    print(f'created_by: {rel.created_by}')\n",
    "    print(f'creation_date: {rel.creation_date}')\n",
    "    print(f'definition: {rel.definition}')\n",
    "    print(f'disjoint_from: {rel.disjoint_from}')\n",
    "    print(f'equivalent_to: {rel.equivalent_to}')\n",
    "    print(f'id: {rel.id}')\n",
    "    print(f'name: {rel.name}')\n",
    "    print(f'namespace: {rel.namespace}')\n",
    "    print(f'obsolete: {rel.obsolete}')\n",
    "    print(f'relationships: {rel.relationships}')\n",
    "    print(f'replaced_by: {rel.replaced_by}')\n",
    "    print(f'subsets: {rel.subsets}')\n",
    "    print(f'synonyms: {rel.synonyms}')\n",
    "    print(f'xrefs: {rel.xrefs}')\n",
    "    print(f'antisymmetric: {rel.antisymmetric}')\n",
    "    print(f'asymmetric: {rel.asymmetric}')\n",
    "    print(f'class_level: {rel.class_level}')\n",
    "    print(f'cyclic: {rel.cyclic}')\n",
    "    print(f'disjoint_over: {rel.disjoint_over}')\n",
    "    print(f'domain: {rel.domain}')\n",
    "    print(f'functional: {rel.functional}')\n",
    "    print(f'inverse_functional: {rel.inverse_functional}')\n",
    "    print(f'holds_over_chain: {rel.holds_over_chain}')\n",
    "    print(f'metadata_tag: {rel.metadata_tag}')\n",
    "    print(f'inverse_of: {rel.inverse_of}')\n",
    "    print(f'intersection_of: {rel.intersection_of}')\n",
    "    print(f'range: {rel.range}')\n",
    "    print(f'reflexive: {rel.reflexive}')\n",
    "    print(f'symmetric: {rel.symmetric}')\n",
    "    print(f'transitive: {rel.transitive}')\n",
    "    print(f'transitive_over: {rel.transitive_over}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Edges [indexes and data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Refactored EdgesContainer: does NOT store terms ---\n",
    "class EdgesContainer:\n",
    "    def __init__(self, terms: list, lookup_tables: LookUpTables):\n",
    "        self.edges_indices = self._populate_index_containers(\n",
    "            terms, lookup_tables\n",
    "        )\n",
    "        self.relations = list(self.edges_indices.keys())\n",
    "\n",
    "    # Search all possible relations in the ontology\n",
    "    def _get_ontology_relationships(self, terms):\n",
    "        set_relations = set()\n",
    "        for term in terms:\n",
    "            for rel in term.relationships:\n",
    "                rel_name = rel.name.lower().replace(' ', '_')\n",
    "                set_relations.add(rel_name)\n",
    "        return sorted(set_relations)\n",
    "\n",
    "    # Create empty containers for each relation type\n",
    "    def _create_edges_index_containers(self, terms):\n",
    "        relationships = self._get_ontology_relationships(terms)\n",
    "\n",
    "        # Always include 'is_a' relationship\n",
    "        relationships.append('is_a')\n",
    "        edge_container = {\n",
    "            rel: {'rows': [], 'cols': []} for rel in relationships\n",
    "        }\n",
    "        return edge_container\n",
    "\n",
    "    # Populate the containers with row and column indices\n",
    "    def _populate_index_containers(self, terms, lookup_tables):\n",
    "        edge_container = self._create_edges_index_containers(terms)\n",
    "        for term in terms:\n",
    "            # Populate 'is_a' relationships\n",
    "            for subclass in term.subclasses(with_self=False, distance=1):\n",
    "                if subclass.obsolete:\n",
    "                    continue\n",
    "                rel_name = 'is_a'\n",
    "                edge_container[rel_name]['rows'].append(\n",
    "                    lookup_tables.term_to_index(subclass.id)\n",
    "                )\n",
    "                edge_container[rel_name]['cols'].append(\n",
    "                    lookup_tables.term_to_index(term.id)\n",
    "                )\n",
    "\n",
    "            # Populate other relationships\n",
    "            for rel, targets in term.relationships.items():\n",
    "                rel_name = rel.name.lower().replace(' ', '_')\n",
    "                for target in targets:\n",
    "                    if target.obsolete:\n",
    "                        continue\n",
    "                    edge_container[rel_name]['rows'].append(\n",
    "                        lookup_tables.term_to_index(term.id)\n",
    "                    )\n",
    "                    edge_container[rel_name]['cols'].append(\n",
    "                        lookup_tables.term_to_index(target.id)\n",
    "                    )\n",
    "\n",
    "        # Convert lists to numpy arrays with dtype np.int64\n",
    "        for rel, data in edge_container.items():\n",
    "            data['rows'] = np.array(data['rows'], dtype=np.int64)\n",
    "            data['cols'] = np.array(data['cols'], dtype=np.int64)\n",
    "        return edge_container\n",
    "\n",
    "\n",
    "class EdgesDataframe:\n",
    "    def __init__(self, terms, include_obsolete=False):\n",
    "        self.dataframe = self.create_edges_dataframe(\n",
    "            terms, include_obsolete=include_obsolete\n",
    "        )\n",
    "\n",
    "    def __get_last_part_string(self, s, separators=('/', ':', '.', '#')):\n",
    "        pattern = '|'.join(map(re.escape, separators))\n",
    "        parts = re.split(pattern, s)\n",
    "        return parts[-1] if parts else s\n",
    "\n",
    "    def create_edges_dataframe(self, terms, include_obsolete=False):\n",
    "        \"\"\"Create a DataFrame with fields: source_id, source_name, relation, target_id, target_name, is_obsolete.\"\"\"\n",
    "        rows = []\n",
    "        for term in terms:\n",
    "            if not include_obsolete and term.obsolete:\n",
    "                continue\n",
    "            source_id = term.id\n",
    "            source_name = term.name\n",
    "            for rel, targets in term.relationships.items():\n",
    "                rel_name = rel.name\n",
    "                for target in targets:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'source_id': source_id,\n",
    "                            'source_name': source_name,\n",
    "                            'relation': rel_name,\n",
    "                            'target_id': target.id,\n",
    "                            'target_name': target.name,\n",
    "                            'is_obsolete': target.obsolete,\n",
    "                        }\n",
    "                    )\n",
    "            # Add is_a relationships (subclasses)\n",
    "            for subclass in term.subclasses(with_self=False, distance=1):\n",
    "                if not include_obsolete and subclass.obsolete:\n",
    "                    continue\n",
    "                rows.append(\n",
    "                    {\n",
    "                        'source_id': subclass.id,\n",
    "                        'source_name': subclass.name,\n",
    "                        'relation': 'is_a',\n",
    "                        'target_id': term.id,\n",
    "                        'target_name': term.name,\n",
    "                        'is_obsolete': subclass.obsolete,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.sort_values(['source_id', 'relation', 'target_id'], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.insert(0, 'index', range(len(df)))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df = EdgesDataframe(go_ontology.terms())\n",
    "e_df.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Ontology as a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes_indexes,\n",
    "        nodes_dataframe,\n",
    "        edges_indexes,\n",
    "        edges_dataframe,\n",
    "        lookup_tables,\n",
    "    ):\n",
    "        # --- Nodes ---\n",
    "        self.nodes_indexes = nodes_indexes\n",
    "        self.nodes_dataframe = nodes_dataframe\n",
    "        # --- Edges ---\n",
    "        self.edges_indexes = edges_indexes\n",
    "        self.edges_dataframe = edges_dataframe\n",
    "\n",
    "        # --- Lookup Tables ---\n",
    "        self.lookup_tables = lookup_tables\n",
    "        self.number_nodes = len(self.nodes_indexes)\n",
    "        self.number_edges = len(\n",
    "            self.edges_indexes.edges_indices['is_a']['rows']\n",
    "        )\n",
    "\n",
    "        self.matrices_container = self.create_multiple_matrices(\n",
    "            edge_container=self.edges_indexes.edges_indices,\n",
    "            nrows=self.number_nodes,\n",
    "            ncols=self.number_nodes,\n",
    "        )\n",
    "\n",
    "    def create_graphblas_matrix(\n",
    "        self, rows_indexes, cols_indexes, nrows, ncols, name\n",
    "    ):\n",
    "        M = gb.Matrix.from_coo(\n",
    "            rows=rows_indexes,\n",
    "            columns=cols_indexes,\n",
    "            values=1.0,\n",
    "            nrows=nrows,\n",
    "            ncols=ncols,\n",
    "            dtype=bool,\n",
    "            name=name,\n",
    "        )\n",
    "        return M\n",
    "\n",
    "    def create_multiple_matrices(self, edge_container, nrows, ncols):\n",
    "        matrices = {}\n",
    "        for relation, indexes in edge_container.items():\n",
    "            M = self.create_graphblas_matrix(\n",
    "                rows_indexes=indexes['rows'],\n",
    "                cols_indexes=indexes['cols'],\n",
    "                nrows=nrows,\n",
    "                ncols=ncols,\n",
    "                name=relation,\n",
    "            )\n",
    "\n",
    "            matrices[relation] = M\n",
    "        return matrices\n",
    "\n",
    "    ## Graph Operations\n",
    "\n",
    "    # Generates a one-hot encoded vector for a given index\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def one_hot_vector(self, index: int) -> gb.Vector:\n",
    "        return gb.Vector.from_coo(\n",
    "            [index], [1], size=self.number_nodes, dtype=int\n",
    "        )\n",
    "\n",
    "    # -- get_children(term_id, include_self=False)\n",
    "    def get_children(self, term_id, include_self=False):\n",
    "        # validate and resolve the index\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "\n",
    "        # Initialize a one-hot vector for the term node\n",
    "        vector_node = self.one_hot_vector(index=index)\n",
    "\n",
    "        # Propagate to children using matrix-vector multiplication\n",
    "        children_vec = (self.matrices_container['is_a'] @ vector_node).new()\n",
    "\n",
    "        # Optionally include the node itself\n",
    "        if include_self:\n",
    "            children_vec[index] = True\n",
    "\n",
    "        # translate indexes to terms\n",
    "        terms = [term for term in children_vec]\n",
    "\n",
    "        return self.lookup_tables.index_to_term(terms)\n",
    "\n",
    "    # -- get_parents(term_id, include_self=False)\n",
    "    def get_parents(self, term_id, include_self=False):\n",
    "        # validate and resolve the index\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "\n",
    "        # Initialize a one-hot vector for the term node\n",
    "        vector_node = self.one_hot_vector(index=index)\n",
    "\n",
    "        # Propagate to children using matrix-vector multiplication\n",
    "        parent_vec = (self.matrices_container['is_a'].T @ vector_node).new()\n",
    "\n",
    "        # Optionally include the node itself\n",
    "        if include_self:\n",
    "            parent_vec[index] = True\n",
    "\n",
    "        # translate indexes to terms\n",
    "        terms = [term for term in parent_vec]\n",
    "\n",
    "        return self.lookup_tables.index_to_term(terms)\n",
    "\n",
    "    # -- get_root()\n",
    "    def get_root(self):\n",
    "        matrix = self.matrices_container['is_a'].T\n",
    "\n",
    "        # 1. Compute the number of incoming edges per node (column-wise sum)\n",
    "        col_sums_expr = matrix.reduce_columnwise(gb.binary.plus)\n",
    "\n",
    "        # 2. Materialize the VectorExpression\n",
    "        col_sums_vec = col_sums_expr.new()\n",
    "\n",
    "        # 3. Extract non-zero indices and their counts\n",
    "        indices, values = col_sums_vec.to_coo()\n",
    "\n",
    "        # 4. Create dense array of incoming edge counts\n",
    "        col_sums_np = np.zeros(matrix.ncols, dtype=np.int64)\n",
    "        col_sums_np[indices] = values\n",
    "\n",
    "        # 5. Roots = nodes with zero incoming edges\n",
    "        roots = np.where(col_sums_np == 0)[0]\n",
    "\n",
    "        return self.lookup_tables.index_to_term(roots)\n",
    "\n",
    "    def _traverse_graph(\n",
    "        self, term_id, adjacency_matrix, distance=None, include_self=False\n",
    "    ):\n",
    "        \"\"\"Generalized function to traverse a graph in either direction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The starting term ID.\n",
    "        adjacency_matrix : gb.Matrix\n",
    "            Adjacency matrix to traverse (forward for descendants, transposed for ancestors).\n",
    "        distance : int or None\n",
    "            Maximum distance to traverse. None means unlimited.\n",
    "        include_self : bool\n",
    "            Whether to include the starting node in the result.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs reached.\n",
    "        \"\"\"\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        index = self.lookup_tables.term_to_index(term_id)\n",
    "        current_vector = self.one_hot_vector(index=index)\n",
    "        visited = set()\n",
    "\n",
    "        if include_self:\n",
    "            visited.add(index)\n",
    "\n",
    "        while current_vector.nvals != 0 and distance != 0:\n",
    "            next_vector = gb.Vector(dtype=int, size=adjacency_matrix.nrows)\n",
    "            next_vector << gb.semiring.plus_times(\n",
    "                adjacency_matrix @ current_vector\n",
    "            )  # forward or transposed depends on matrix\n",
    "\n",
    "            next_indices = set(next_vector.to_coo()[0])\n",
    "            next_indices.difference_update(visited)\n",
    "\n",
    "            if not next_indices:\n",
    "                break\n",
    "\n",
    "            visited.update(next_indices)\n",
    "            current_vector = gb.Vector.from_coo(\n",
    "                list(next_indices),\n",
    "                [1] * len(next_indices),\n",
    "                size=adjacency_matrix.nrows,\n",
    "            )\n",
    "\n",
    "            if distance is not None:\n",
    "                distance -= 1\n",
    "\n",
    "        return self.lookup_tables.index_to_term(list(visited))\n",
    "\n",
    "    # Public API functions\n",
    "    def get_ancestors(self, term_id, distance=None, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container[\n",
    "            'is_a'\n",
    "        ].T  # transpose for ancestors\n",
    "        return self._traverse_graph(\n",
    "            term_id, adjacency_matrix, distance, include_self\n",
    "        )\n",
    "\n",
    "    def get_descendants(self, term_id, distance=None, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container[\n",
    "            'is_a'\n",
    "        ]  # normal direction for descendants\n",
    "        return self._traverse_graph(\n",
    "            term_id, adjacency_matrix, distance, include_self\n",
    "        )\n",
    "\n",
    "    def _traverse_graph_with_distance(\n",
    "        self, term_id, adjacency_matrix, include_self=False\n",
    "    ):\n",
    "        \"\"\"Generalized function to traverse a graph and return nodes with distance from start.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The starting term ID.\n",
    "        adjacency_matrix : gb.Matrix\n",
    "            Adjacency matrix to traverse (forward for descendants, transposed for ancestors).\n",
    "        include_self : bool\n",
    "            Whether to include the starting node with distance 0.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[Tuple[int, int]]\n",
    "            List of tuples (node_index, distance_from_start)\n",
    "        \"\"\"\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        start_index = self.lookup_tables.term_to_index(term_id)\n",
    "        current_vector = self.one_hot_vector(index=start_index)\n",
    "\n",
    "        distances = {}  # {node_index: distance}\n",
    "        distance_counter = 0\n",
    "\n",
    "        if include_self:\n",
    "            distances[start_index] = 0\n",
    "\n",
    "        while current_vector.nvals != 0:\n",
    "            next_vector = gb.Vector(dtype=int, size=adjacency_matrix.nrows)\n",
    "            next_vector << gb.semiring.plus_times(\n",
    "                adjacency_matrix @ current_vector\n",
    "            )\n",
    "\n",
    "            next_indices = set(next_vector.to_coo()[0])\n",
    "            # remove already visited nodes\n",
    "            next_indices.difference_update(distances.keys())\n",
    "\n",
    "            if not next_indices:\n",
    "                break\n",
    "\n",
    "            distance_counter += 1\n",
    "            for idx in next_indices:\n",
    "                distances[idx] = distance_counter\n",
    "\n",
    "            current_vector = gb.Vector.from_coo(\n",
    "                list(next_indices),\n",
    "                [1] * len(next_indices),\n",
    "                size=adjacency_matrix.nrows,\n",
    "            )\n",
    "\n",
    "        # return as list of tuples\n",
    "        return [\n",
    "            (self.lookup_tables.index_to_term(int(index)), distance)\n",
    "            for index, distance in distances.items()\n",
    "        ]\n",
    "\n",
    "    # Public API functions\n",
    "    def get_ancestors_with_distance(self, term_id, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container[\n",
    "            'is_a'\n",
    "        ].T  # transpose for ancestors\n",
    "        return self._traverse_graph_with_distance(\n",
    "            term_id, adjacency_matrix, include_self\n",
    "        )\n",
    "\n",
    "    def get_descendants_with_distance(self, term_id, include_self=False):\n",
    "        adjacency_matrix = self.matrices_container[\n",
    "            'is_a'\n",
    "        ]  # normal direction for descendants\n",
    "        return self._traverse_graph_with_distance(\n",
    "            term_id, adjacency_matrix, include_self\n",
    "        )\n",
    "\n",
    "    def get_common_ancestors(self, node_ids):\n",
    "        \"\"\"Return the common ancestors of a list of terms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : List[str]\n",
    "            List of starting term IDs.\n",
    "        include_self : bool\n",
    "            Whether to include the starting nodes themselves in the ancestor sets.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs that are common ancestors to all input terms.\n",
    "        \"\"\"\n",
    "        if not node_ids:\n",
    "            return []\n",
    "\n",
    "        # get ancestors for the first node\n",
    "        common_ancestors = set(\n",
    "            self.get_ancestors(node_ids[0], include_self=False)\n",
    "        )\n",
    "\n",
    "        # intersect with ancestors of the rest\n",
    "        for term_id in node_ids[1:]:\n",
    "            ancestors = set(self.get_ancestors(term_id, include_self=False))\n",
    "            common_ancestors.intersection_update(ancestors)\n",
    "\n",
    "            # early exit if no common ancestor remains\n",
    "            if not common_ancestors:\n",
    "                return []\n",
    "\n",
    "        return set(common_ancestors)\n",
    "\n",
    "    def get_lowest_common_ancestors(self, node_ids):\n",
    "        \"\"\"Return the lowest common ancestor(s) of a list of terms.\n",
    "        Lowest = closest to the given terms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : List[str]\n",
    "            List of starting term IDs.\n",
    "        include_self : bool\n",
    "            Whether to include the starting nodes in ancestor sets.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs that are the lowest common ancestors.\n",
    "        \"\"\"\n",
    "        if not node_ids:\n",
    "            return []\n",
    "\n",
    "        # Compute ancestors with distances for the first node\n",
    "        first_ancestors = dict(\n",
    "            self.get_ancestors_with_distance(node_ids[0], include_self=False)\n",
    "        )\n",
    "        common_ancestors = set(first_ancestors.keys())\n",
    "\n",
    "        # Initialize distances dict for LCA calculation\n",
    "        # key: ancestor index, value: max distance from any node\n",
    "        lca_distances = {idx: dist for idx, dist in first_ancestors.items()}\n",
    "\n",
    "        # Process remaining nodes\n",
    "        for term_id in node_ids[1:]:\n",
    "            ancestors_with_distance = dict(\n",
    "                self.get_ancestors_with_distance(term_id, include_self=False)\n",
    "            )\n",
    "            ancestors_set = set(ancestors_with_distance.keys())\n",
    "            common_ancestors.intersection_update(ancestors_set)\n",
    "\n",
    "            # Update max distance for each common ancestor\n",
    "            lca_distances = {\n",
    "                idx: max(lca_distances[idx], ancestors_with_distance[idx])\n",
    "                for idx in common_ancestors\n",
    "            }\n",
    "\n",
    "            # Early exit if no common ancestor remains\n",
    "            if not common_ancestors:\n",
    "                return []\n",
    "\n",
    "        if not lca_distances:\n",
    "            return []\n",
    "\n",
    "        # Find the minimum of the maximum distances\n",
    "        min_distance = min(lca_distances.values())\n",
    "\n",
    "        # Return ancestor IDs that have this minimum distance\n",
    "        lowest_common_indices = [\n",
    "            idx for idx, dist in lca_distances.items() if dist == min_distance\n",
    "        ]\n",
    "        return lowest_common_indices\n",
    "\n",
    "    def get_distance_from_root(self, term_id):\n",
    "        \"\"\"Calculate the distance from the given term to the root node(s) of the ontology.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The term ID for which to compute the distance from root.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            Distance from the term to the root (number of edges).\n",
    "            Returns 0 if the term is a root itself.\n",
    "        \"\"\"\n",
    "        # Validate term\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        # Get all ancestors with distance\n",
    "        ancestors_with_distance = self.get_ancestors_with_distance(\n",
    "            term_id, include_self=True\n",
    "        )\n",
    "\n",
    "        if not ancestors_with_distance:\n",
    "            # No ancestors, this term is a root\n",
    "            return 0\n",
    "\n",
    "        # Distance from root = maximum distance in the ancestors path\n",
    "        max_distance = max(distance for _, distance in ancestors_with_distance)\n",
    "\n",
    "        return max_distance\n",
    "\n",
    "    def get_path_between(self, node_a, node_b):\n",
    "        \"\"\"Find the shortest path between two nodes in the ontology.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_a : str\n",
    "            Starting term ID.\n",
    "        node_b : str\n",
    "            Ending term ID.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            List of term IDs representing the path from node_a to node_b (inclusive).\n",
    "            Returns empty list if no path exists.\n",
    "        \"\"\"\n",
    "        if node_a not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {node_a}')\n",
    "        if node_b not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {node_b}')\n",
    "\n",
    "        # Check if a path exists\n",
    "        if not (\n",
    "            self.is_ancestor(node_a, node_b)\n",
    "            or self.is_descendant(node_a, node_b)\n",
    "        ):\n",
    "            return []\n",
    "\n",
    "        # Determine direction\n",
    "        if self.is_ancestor(node_a, node_b):\n",
    "            start, end = node_a, node_b\n",
    "            adjacency_matrix = self.matrices_container['is_a']\n",
    "        else:\n",
    "            start, end = node_b, node_a\n",
    "            adjacency_matrix = self.matrices_container['is_a']\n",
    "\n",
    "        start_idx = self.lookup_tables.term_to_index(start)\n",
    "        end_idx = self.lookup_tables.term_to_index(end)\n",
    "\n",
    "        # BFS to find shortest path\n",
    "        from collections import deque\n",
    "\n",
    "        queue = deque([[start_idx]])\n",
    "        visited = set([start_idx])\n",
    "\n",
    "        while queue:\n",
    "            path = queue.popleft()\n",
    "            current = path[-1]\n",
    "\n",
    "            if current == end_idx:\n",
    "                return self.lookup_tables.index_to_term(path)\n",
    "\n",
    "            # Get children (or parents depending on direction)\n",
    "            neighbors_vec = adjacency_matrix @ self.one_hot_vector(current)\n",
    "            neighbors = neighbors_vec.to_coo()[0]\n",
    "\n",
    "            for n in neighbors:\n",
    "                if n not in visited:\n",
    "                    visited.add(n)\n",
    "                    queue.append(path + [n])\n",
    "\n",
    "        return []\n",
    "\n",
    "    def is_ancestor(self, ancestor_node, descendant_node):\n",
    "        \"\"\"Check if `ancestor_node` is an ancestor of `descendant_node`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ancestor_node : str\n",
    "            Candidate ancestor term ID.\n",
    "        descendant_node : str\n",
    "            Candidate descendant term ID.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        bool\n",
    "            True if `ancestor_node` is an ancestor of `descendant_node`, else False.\n",
    "        \"\"\"\n",
    "        if descendant_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {descendant_node}')\n",
    "        if ancestor_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {ancestor_node}')\n",
    "\n",
    "        # Retrieve ancestors of the descendant\n",
    "        ancestors = set(self.get_ancestors(descendant_node, include_self=False))\n",
    "        return ancestor_node in ancestors\n",
    "\n",
    "    def is_descendant(self, descendant_node, ancestor_node):\n",
    "        \"\"\"Check if `descendant_node` is a descendant of `ancestor_node`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        descendant_node : str\n",
    "            Candidate descendant term ID.\n",
    "        ancestor_node : str\n",
    "            Candidate ancestor term ID.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        bool\n",
    "            True if `descendant_node` is a descendant of `ancestor_node`, else False.\n",
    "        \"\"\"\n",
    "        if ancestor_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {ancestor_node}')\n",
    "        if descendant_node not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {descendant_node}')\n",
    "\n",
    "        # Retrieve descendants of the ancestor\n",
    "        descendants = set(\n",
    "            self.get_descendants(ancestor_node, include_self=False)\n",
    "        )\n",
    "        return descendant_node in descendants\n",
    "\n",
    "    def get_siblings(self, term_id, include_self: bool = False):\n",
    "        \"\"\"Retrieve all siblings of a given term (i.e., nodes that share at least one parent).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        term_id : str\n",
    "            The term ID whose siblings are to be found.\n",
    "        include_self : bool, optional (default=False)\n",
    "            Whether to include the term itself in the returned set.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            List of sibling term IDs.\n",
    "        \"\"\"\n",
    "        # Validate term existence\n",
    "        if term_id not in self.lookup_tables.get_lut_term_to_index():\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        # Step 1: Get parents of the given term\n",
    "        parents = self.get_parents(term_id, include_self=False)\n",
    "        if not parents:\n",
    "            # No parents means this term is a root -> no siblings\n",
    "            return []\n",
    "\n",
    "        # Step 2: For each parent, get its children\n",
    "        siblings_set = set()\n",
    "        for parent_id in parents:\n",
    "            children = self.get_children(parent_id, include_self=False)\n",
    "            siblings_set.update(children)\n",
    "\n",
    "        # Step 3: Optionally remove the term itself\n",
    "        if not include_self and term_id in siblings_set:\n",
    "            siblings_set.remove(term_id)\n",
    "\n",
    "        # Return as sorted list for deterministic output\n",
    "        return sorted(siblings_set)\n",
    "\n",
    "    def is_sibling(self, node_a: str, node_b: str) -> bool:\n",
    "        \"\"\"Check if two nodes are siblings (i.e., share at least one common parent).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_a : str\n",
    "            First node (term ID).\n",
    "        node_b : str\n",
    "            Second node (term ID).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        bool\n",
    "            True if both nodes share at least one parent; False otherwise.\n",
    "        \"\"\"\n",
    "        # Validate existence\n",
    "        lut = self.lookup_tables.get_lut_term_to_index()\n",
    "        if node_a not in lut:\n",
    "            raise KeyError(f'Unknown term ID: {node_a}')\n",
    "        if node_b not in lut:\n",
    "            raise KeyError(f'Unknown term ID: {node_b}')\n",
    "\n",
    "        # Step 1: Get parents for both nodes\n",
    "        parents_a = set(self.get_parents(node_a, include_self=False))\n",
    "        parents_b = set(self.get_parents(node_b, include_self=False))\n",
    "\n",
    "        # Step 2: Intersection of parents indicates sibling relationship\n",
    "        shared_parents = parents_a.intersection(parents_b)\n",
    "\n",
    "        # Step 3: Return True if they share any parent\n",
    "        return len(shared_parents) > 0\n",
    "\n",
    "    def get_trajectories_from_root(self, term_id: str) -> list[list[dict]]:\n",
    "        \"\"\"Get all ancestor trajectories from the root(s) to the given term using GraphBLAS operations.\n",
    "\n",
    "        Args:\n",
    "            term_id (str): The identifier of the term.\n",
    "\n",
    "        Returns:\n",
    "            list[list[dict]]: List of trajectories; each trajectory is a list of dictionaries\n",
    "                            with keys: 'id', 'name', and 'distance' (from the queried term).\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        lut_term_to_index = self.lookup_tables.get_lut_term_to_index()\n",
    "        if term_id not in lut_term_to_index:\n",
    "            raise KeyError(f'Unknown term ID: {term_id}')\n",
    "\n",
    "        A_T = self.matrices_container['is_a'].T\n",
    "        term_idx = int(self.lookup_tables.term_to_index(term_id))\n",
    "\n",
    "        # Root detection\n",
    "        roots = set(self.get_root())\n",
    "        root_indices = {int(self.lookup_tables.term_to_index(r)) for r in roots}\n",
    "\n",
    "        from collections import deque\n",
    "\n",
    "        queue = deque([[term_idx]])\n",
    "        trajectories = []\n",
    "\n",
    "        while queue:\n",
    "            path = queue.popleft()\n",
    "            current_idx = int(path[0])\n",
    "\n",
    "            # Parent discovery using GraphBLAS multiplication\n",
    "            parent_vec = (A_T @ self.one_hot_vector(current_idx)).new()\n",
    "            parent_indices = [int(i) for i in parent_vec.to_coo()[0]]\n",
    "\n",
    "            # Termination condition: reached a root or no parents\n",
    "            if not parent_indices or current_idx in root_indices:\n",
    "                # Reverse path  root  term order\n",
    "                reversed_path = list(reversed(path))\n",
    "                traj = []\n",
    "                for dist, idx in enumerate(\n",
    "                    reversed_path[::-1]\n",
    "                ):  # distance from term\n",
    "                    idx = int(idx)\n",
    "                    traj.append(\n",
    "                        {\n",
    "                            'id': self.lookup_tables.index_to_term(idx),\n",
    "                            'name': self.lookup_tables.term_to_description(\n",
    "                                self.lookup_tables.index_to_term(idx)\n",
    "                            ),\n",
    "                            'distance': dist,\n",
    "                        }\n",
    "                    )\n",
    "                trajectories.append(\n",
    "                    list(reversed(traj))\n",
    "                )  # ensure rootterm order\n",
    "            else:\n",
    "                for p in parent_indices:\n",
    "                    if p not in path:\n",
    "                        queue.append([p] + path)\n",
    "\n",
    "        for traj in trajectories:\n",
    "            traj.reverse()  # optional: reverse to have root-first order\n",
    "\n",
    "        return trajectories  # optional: reverse to have root-first order\n",
    "\n",
    "    def print_term_trajectories_tree(self, trajectories: list[dict]) -> None:\n",
    "        \"\"\"Print all ancestor trajectories as a single ASCII tree from root to the original term.\n",
    "\n",
    "        Combining shared nodes.\n",
    "\n",
    "        Args:\n",
    "            trajectories: List of lists, each inner list is a trajectory (branch) as returned by ancestor_trajectories.\n",
    "        \"\"\"\n",
    "        if not trajectories:\n",
    "            print('No trajectories to display.')\n",
    "            return\n",
    "        root = self._build_tree_from_trajectories(trajectories)\n",
    "        self._print_ascii_tree(root)\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_tree_from_trajectories(trajectories: list[dict]) -> object:\n",
    "        \"\"\"Build a tree structure from the list of branches (trajectories).\n",
    "\n",
    "        Returns the root node.\n",
    "\n",
    "        Args:\n",
    "            trajectories (list[dict]): List of trajectory branches.\n",
    "\n",
    "        Returns:\n",
    "            object: The root node of the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        class Node:\n",
    "            def __init__(self, node_id: str, name: str, distance: int) -> None:\n",
    "                self.id = node_id\n",
    "                self.name = name\n",
    "                self.distance = distance\n",
    "                self.children = {}\n",
    "\n",
    "        def insert_branch(root: Node, branch: list) -> None:\n",
    "            node = root\n",
    "            for item in branch:\n",
    "                key = (item['id'], item['name'], item['distance'])\n",
    "                if key not in node.children:\n",
    "                    node.children[key] = Node(*key)\n",
    "                node = node.children[key]\n",
    "\n",
    "        # All branches are sorted from term to root, so reverse to root-to-term\n",
    "        branch_lists = [list(branch) for branch in trajectories]\n",
    "        root_info = branch_lists[0][0]\n",
    "        root = Node(root_info['id'], root_info['name'], root_info['distance'])\n",
    "        for branch in branch_lists:\n",
    "            insert_branch(root, branch[1:])  # skip root itself, already created\n",
    "        return root\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_ascii_tree(root: object) -> None:\n",
    "        \"\"\"Print the tree structure in ASCII format starting from the root node.\"\"\"\n",
    "\n",
    "        def print_ascii_tree(\n",
    "            node: object, prefix: str = '', is_last: bool = True\n",
    "        ) -> None:\n",
    "            connector = ' ' if is_last else ' '\n",
    "            print(\n",
    "                f'{prefix}{connector}{node.id}: {node.name} (distance={node.distance})'\n",
    "            )\n",
    "            child_items = list(node.children.values())\n",
    "            for idx, child in enumerate(child_items):\n",
    "                is_last_child = idx == len(child_items) - 1\n",
    "                next_prefix = prefix + ('    ' if is_last else '   ')\n",
    "                print_ascii_tree(child, next_prefix, is_last_child)\n",
    "\n",
    "        # Print root without prefix\n",
    "        print(f'{root.id}: {root.name} (distance={root.distance})')\n",
    "        child_items = list(root.children.values())\n",
    "        for idx, child in enumerate(child_items):\n",
    "            is_last_child = idx == len(child_items) - 1\n",
    "            print_ascii_tree(child, '', is_last_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Extract ontology info and populate objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(ontology, include_obsolete=False):\n",
    "    \"\"\"Single-pass extraction of pronto.Term objects, sorted by term.id.\"\"\"\n",
    "    terms = [t for t in ontology.terms() if include_obsolete or not t.obsolete]\n",
    "    terms.sort(key=lambda t: t.id)\n",
    "    return terms\n",
    "\n",
    "\n",
    "# --- Refactored extract_data_ontology ---\n",
    "def extract_data_ontology(ontology, include_obsolete=False):\n",
    "    # Extract all the terms in the ontology\n",
    "    terms = extract_terms(ontology, include_obsolete=include_obsolete)\n",
    "\n",
    "    # Extract Lookup Tables and DataFrames\n",
    "    LUTS = LookUpTables(terms=terms)\n",
    "    nodes_df = create_nodes_dataframe(\n",
    "        terms=terms, include_obsolete=include_obsolete\n",
    "    )\n",
    "    nodes_indexes = NodeContainer(\n",
    "        nodes_indices=nodes_df['index'].to_numpy(dtype=np.int64)\n",
    "    )\n",
    "    edges_indexes = EdgesContainer(terms=terms, lookup_tables=LUTS)\n",
    "    edges_df = None\n",
    "\n",
    "    G = Graph(\n",
    "        nodes_indexes=nodes_indexes,\n",
    "        nodes_dataframe=nodes_df,\n",
    "        edges_indexes=edges_indexes,\n",
    "        edges_dataframe=edges_df,\n",
    "        lookup_tables=LUTS,\n",
    "    )\n",
    "\n",
    "    return LUTS, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUTS, G = extract_data_ontology(ontology=go_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = LUTS.term_to_index(G.get_root())\n",
    "print(indexes)\n",
    "\n",
    "eval(G.nodes_dataframe.iloc[indexes[2]].annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Step 3. Delete Ontologies from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del go_ontology, chebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Step 4. Evaluate operations\n",
    "\n",
    "- [x] `get_ancestors`\n",
    "- [x] `get_ancestors_with_distance`\n",
    "- [x] `get_children`\n",
    "- [x] `get_common_ancestors`\n",
    "- [x] `get_descendants`\n",
    "- [x] `get_descendants_with_distance`\n",
    "- [x] `get_distance_from_root`\n",
    "- [x] `get_lowest_common_ancestors`\n",
    "- [x] `get_parents`\n",
    "- [x] `get_path_between`\n",
    "- [x] `get_root`\n",
    "- [x] `get_siblings`\n",
    "- [ ] `get_term`  <--- which information to include?\n",
    "- [x] `get_trajectories_from_root`\n",
    "- [x] `is_ancestor`\n",
    "- [x] `is_descendant`\n",
    "- [x] `is_sibling`\n",
    "- [ ] `load` <--- It should load the graph and the ontology?\n",
    "- [x] `print_term_trajectories_tree`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### `get_ancestors(term_id, distance=None, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_ancestors('GO:0051322', distance=5, include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### `get_ancestors_with_distance(term_id, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_ancestors_with_distance('GO:0051322', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### `get_children(term_id, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_children('GO:0048308', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### `get_common_ancestors(node_ids)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_common_ancestors(['GO:0000092', 'GO:0051325'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### `get_descendants(term_id, distance=None, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_descendants('GO:0051322', distance=5, include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### `get_descendants_with_distance(term_id, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_descendants_with_distance('GO:0051322', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### `get_distance_from_root(term_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_distance_from_root('GO:0000092')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### `get_distance_from_root(term_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_lowest_common_ancestors(['GO:0000092', 'GO:0051325'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### `get_parents(term_id, include_self=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_parents('GO:0048308', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### `get_path_between(node_a, node_b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUTS.term_to_description(G.get_path_between('GO:0044848', 'GO:0000092'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### `get_root()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### `get_siblings(term_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_siblings('GO:0000017', include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### `get_trajectories_from_root(term_id)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_trajectories_from_root('GO:0000017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### `is_ancestor(ancestor_node, descendant_node)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_ancestor('GO:0015759', 'GO:0051325')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_ancestor('GO:0015759', 'GO:0042946')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### `is_descendant(descendant_node, ancestor_node)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_descendant('GO:0015759', 'GO:0042946')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### `is_sibling(node_a, node_b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_sibling('GO:0015759', 'GO:0000017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### `print_term_trajectories_tree(trajectories)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.print_term_trajectories_tree(G.get_trajectories_from_root('GO:0000017'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Human-readable query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List\n",
    "\n",
    "class QueryEngine:\n",
    "    def __init__(self, graph):\n",
    "        \"\"\"Initialize the QueryEngine with a Graph instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : Graph\n",
    "            Your ontology graph instance.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "\n",
    "    # --------------------------\n",
    "    # Parsing\n",
    "    # --------------------------\n",
    "    @staticmethod\n",
    "    def parse_query(query: str) -> List[str]:\n",
    "        \"\"\"Very simple parser: splits query into tokens (terms, AND, OR, NOT, parentheses).\n",
    "\n",
    "        Examples:\n",
    "            \"'actomyosin' AND 'stress fiber'\"\n",
    "            \"(term1 OR term2) AND NOT term3\"\n",
    "        \"\"\"\n",
    "        # Match quoted terms, operators, and parentheses\n",
    "        token_pattern = r\"'[^']+'|\\(|\\)|\\bAND\\b|\\bOR\\b|\\bNOT\\b\"\n",
    "        tokens = re.findall(token_pattern, query, flags=re.IGNORECASE)\n",
    "        return [\n",
    "            t.upper() if t.upper() in {'AND', 'OR', 'NOT'} else t.strip(\"'\")\n",
    "            for t in tokens\n",
    "        ]\n",
    "\n",
    "    # --------------------------\n",
    "    # Postfix conversion (shunting-yard)\n",
    "    # --------------------------\n",
    "    def _infix_to_postfix(self, tokens: List[str]) -> List[str]:\n",
    "        precedence = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "        output = []\n",
    "        stack = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in {'AND', 'OR', 'NOT', '(', ')'}:\n",
    "                output.append(token)\n",
    "            elif token == '(':\n",
    "                stack.append(token)\n",
    "            elif token == ')':\n",
    "                while stack and stack[-1] != '(':\n",
    "                    output.append(stack.pop())\n",
    "                stack.pop()  # remove '('\n",
    "            else:\n",
    "                while (\n",
    "                    stack\n",
    "                    and stack[-1] != '('\n",
    "                    and precedence.get(stack[-1], 0) >= precedence[token]\n",
    "                ):\n",
    "                    output.append(stack.pop())\n",
    "                stack.append(token)\n",
    "\n",
    "        while stack:\n",
    "            output.append(stack.pop())\n",
    "\n",
    "        return output\n",
    "\n",
    "    # --------------------------\n",
    "    # Query operations\n",
    "    # --------------------------\n",
    "    def _operation_AND(self, sets: List[Set[str]]) -> Set[str]:\n",
    "        if not sets:\n",
    "            return set()\n",
    "        result_set = sets[0]\n",
    "        for s in sets[1:]:\n",
    "            result_set = result_set.intersection(s)\n",
    "        return result_set\n",
    "\n",
    "    def _operation_OR(self, sets: List[Set[str]]) -> Set[str]:\n",
    "        result_set = set()\n",
    "        for s in sets:\n",
    "            result_set.update(s)\n",
    "        return result_set\n",
    "\n",
    "    def _operation_NOT(\n",
    "        self, base_set: Set[str], term_set: Set[str]\n",
    "    ) -> Set[str]:\n",
    "        return base_set - term_set\n",
    "\n",
    "    # --------------------------\n",
    "    # Postfix evaluation\n",
    "    # --------------------------\n",
    "    def _eval_postfix(self, postfix: List[str]) -> Set[str]:\n",
    "        stack = []\n",
    "        for token in postfix:\n",
    "            if token in {'AND', 'OR'}:\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == 'AND':\n",
    "                    stack.append(self._operation_AND([left, right]))\n",
    "                else:\n",
    "                    stack.append(self._operation_OR([left, right]))\n",
    "            elif token == 'NOT':\n",
    "                operand = stack.pop()\n",
    "                # Universe = all nodes in the graph\n",
    "                universe = set(self.graph.nodes_container)\n",
    "                stack.append(self._operation_NOT(universe, operand))\n",
    "            else:\n",
    "                # Convert term to descendants set\n",
    "                term_id = self.graph.lookup_tables.description_to_term(token)\n",
    "                descendants = set(\n",
    "                    self.graph.get_descendants(term_id, include_self=False)\n",
    "                )\n",
    "                stack.append(descendants)\n",
    "\n",
    "        if len(stack) != 1:\n",
    "            raise ValueError(\n",
    "                'Malformed query. Check parentheses and operators.'\n",
    "            )\n",
    "        return stack[0]\n",
    "\n",
    "    # --------------------------\n",
    "    # Public API\n",
    "    # --------------------------\n",
    "    def execute_query(self, query: str) -> Set[str]:\n",
    "        tokens = self.parse_query(query)\n",
    "        postfix = self._infix_to_postfix(tokens)\n",
    "        result = self._eval_postfix(postfix)\n",
    "        return result\n",
    "\n",
    "    def format_results(self, result_set: Set[str]) -> List[str]:\n",
    "        return sorted(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUTS.term_to_description(G.get_children(LUTS.description_to_term('actomyosin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = QueryEngine(G)\n",
    "\n",
    "query1 = (\n",
    "    \"'striated muscle myosin thick filament assembly' AND 'cellular process'\"\n",
    ")\n",
    "results1 = qe.execute_query(query1)\n",
    "print(qe.format_results(results1))\n",
    "print(LUTS.term_to_description(qe.format_results(results1)))\n",
    "\n",
    "query2 = (\n",
    "    \"'striated muscle myosin thick filament assembly' OR 'cellular process'\"\n",
    ")\n",
    "results2 = qe.execute_query(query2)\n",
    "print('\\n', qe.format_results(results2))\n",
    "print(LUTS.term_to_description(qe.format_results(results2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "LUTS.term_to_description(['GO:0030241'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_lipids = pronto.Ontology('../data/out/swiss_lipids_ontology.obo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, G = extract_data_ontology(ontology=swiss_lipids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.term_to_description('SLM:000389806')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.get_ancestors_with_distance('SLM:000389806')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.term_to_description(G.get_ancestors('SLM:000389806'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.print_term_trajectories_tree(G.get_trajectories_from_root('SLM:000389806'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = QueryEngine(G)\n",
    "\n",
    "query1 = \"'Fatty acid esters' AND 'Lipid'\"\n",
    "results1 = qe.execute_query(query1)\n",
    "print(qe.format_results(results1))\n",
    "\n",
    "for result in L.term_to_description(qe.format_results(results1)):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../data/out/nodes_edges_container.h5', 'w') as f:\n",
    "    # Create a group for edges\n",
    "    grp_edges = f.create_group('edges')\n",
    "    for rel, data in G.edges_indexes.edges_indices.items():\n",
    "        rel_grp = grp_edges.create_group(rel)\n",
    "        rel_grp.create_dataset('rows', data=data['rows'], compression='gzip')\n",
    "        rel_grp.create_dataset('cols', data=data['cols'], compression='gzip')\n",
    "\n",
    "    # Create a group for nodes\n",
    "    grp_nodes = f.create_group('nodes')\n",
    "    grp_nodes.create_dataset(\n",
    "        'indices', data=G.nodes_indexes.nodes_indices, compression='gzip'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
